<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Press Start 2P:300,300italic,400,400italic,700,700italic|Noto Serif SC:300,300italic,400,400italic,700,700italic|ZCOOL XiaoWei:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"gh4bo.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="看了PyTorch提供的教程文档，里面有一个PyTorch示例，跟着做一做，顺便学习一下。">
<meta property="og:type" content="article">
<meta property="og:title" content="通过示例学习 PyTorch">
<meta property="og:url" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/index.html">
<meta property="og:site_name" content="Bo">
<meta property="og:description" content="看了PyTorch提供的教程文档，里面有一个PyTorch示例，跟着做一做，顺便学习一下。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/pytorch_warmup.png">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/20180808152358945">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/pytorch_warmup_learningrate.png">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/20200701200703714.png">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/20200701200720530.png">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/20200701200736103.png">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/v2-95729ebb10269f807b0809fb09b125d0_1440w.jpg">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/v2-1781041624f4c9fb31df04d11dd6a84a_1440w.jpg">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/v2-18add4601e35e4b26fb73a50245e8de7_1440w.jpg">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/image-20210420220025404.png">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/autograd.png">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/20160922171529212">
<meta property="og:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/20160922172202855">
<meta property="article:published_time" content="2021-04-15T14:03:06.000Z">
<meta property="article:modified_time" content="2021-04-22T09:17:31.493Z">
<meta property="article:author" content="Bo">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/pytorch_warmup.png">

<link rel="canonical" href="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>通过示例学习 PyTorch | Bo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Bo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">4</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">14</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">14</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/GH4Bo" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://gh4bo.github.io/%E6%89%BF/pytorch-samples/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Bo">
      <meta itemprop="description" content="I can do all things through Christ who strengthens me!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          通过示例学习 PyTorch
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-04-15 22:03:06" itemprop="dateCreated datePublished" datetime="2021-04-15T22:03:06+08:00">2021-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-04-22 17:17:31" itemprop="dateModified" datetime="2021-04-22T17:17:31+08:00">2021-04-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%89%BF/" itemprop="url" rel="index"><span itemprop="name">承</span></a>
                </span>
            </span>

          
            <span id="/%E6%89%BF/pytorch-samples/" class="post-meta-item leancloud_visitors" data-flag-title="通过示例学习 PyTorch" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/%E6%89%BF/pytorch-samples/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/%E6%89%BF/pytorch-samples/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>39k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>35 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>  看了PyTorch提供的教程文档，里面有一个PyTorch示例，跟着做一做，顺便学习一下。</p>
<span id="more"></span>

<p>[TOC]</p>
<h1 id="热身：NumPy"><a href="#热身：NumPy" class="headerlink" title="热身：NumPy"></a>热身：NumPy</h1><p>经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测<code>y = sin(x)</code>从<code>-pi</code>到<code>pi</code>。</p>
<p>此实现使用 numpy 手动计算正向传播，损失和后向通过。</p>
<p>numpy 数组是通用的 n 维数组； 它对深度学习，梯度或计算图一无所知，而只是执行通用数值计算的一种方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line"><span class="comment"># 创建随机输入输出数据</span></span><br><span class="line">x = np.linspace(-math.pi, math.pi, <span class="number">2000</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line"><span class="comment"># 随机初始化权重 有四个权重</span></span><br><span class="line">a = np.random.randn()</span><br><span class="line">b = np.random.randn()</span><br><span class="line">c = np.random.randn()</span><br><span class="line">d = np.random.randn()</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span>	<span class="comment">#学习率</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    <span class="comment"># 前向传播：计算y的值</span></span><br><span class="line">    <span class="comment"># y = a + b x + c x^2 + d x^3</span></span><br><span class="line">    <span class="comment"># 我在这里把它理解成泰勒展开式，展开的越多的话应该越精确</span></span><br><span class="line">    y_pred = a + b * x + c * x ** <span class="number">2</span> + d * x ** <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    <span class="comment"># 计算并且输出损失 y(预测)-y(实际)之差的平方</span></span><br><span class="line">    loss = np.square(y_pred - y).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of a, b, c, d with respect to loss</span></span><br><span class="line">    <span class="comment"># 反向传播计算a,b,c,d相对于损失的梯度</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_a = grad_y_pred.<span class="built_in">sum</span>()</span><br><span class="line">    grad_b = (grad_y_pred * x).<span class="built_in">sum</span>()</span><br><span class="line">    grad_c = (grad_y_pred * x ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    grad_d = (grad_y_pred * x ** <span class="number">3</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    <span class="comment"># 更新权重</span></span><br><span class="line">    a -= learning_rate * grad_a</span><br><span class="line">    b -= learning_rate * grad_b</span><br><span class="line">    c -= learning_rate * grad_c</span><br><span class="line">    d -= learning_rate * grad_d</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;Result: y = <span class="subst">&#123;a&#125;</span> + <span class="subst">&#123;b&#125;</span> x + <span class="subst">&#123;c&#125;</span> x^2 + <span class="subst">&#123;d&#125;</span> x^3&#x27;</span>)	<span class="comment">#得出sinx的三阶多项式</span></span><br></pre></td></tr></table></figure>

<p>接下来绘制函数，查看差别</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">i = np.arange(-math.pi, math.pi, <span class="number">0.1</span>)</span><br><span class="line">j = a + b * i + c * i ** <span class="number">2</span> + d * i ** <span class="number">3</span></span><br><span class="line">k = np.sin(i)</span><br><span class="line">plt.plot(i,j,label = <span class="string">&quot;y_pred&quot;</span>)</span><br><span class="line">plt.plot(i,k,label = <span class="string">&#x27;y&#x27;</span>,linestyle = <span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>这里给出输出结果，由于存在随机数，数字可能有所差别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.7972626441826002</span> -<span class="number">1.798680298424662</span> -<span class="number">1.5276217691324827</span> <span class="number">2.4007340683267087</span></span><br><span class="line"><span class="number">99</span> <span class="number">6770.849592635585</span></span><br><span class="line"><span class="number">199</span> <span class="number">4511.314696929819</span></span><br><span class="line"><span class="number">299</span> <span class="number">3008.1063132839636</span></span><br><span class="line"><span class="number">399</span> <span class="number">2007.6743709025282</span></span><br><span class="line"><span class="number">499</span> <span class="number">1341.5847436481672</span></span><br><span class="line"><span class="number">599</span> <span class="number">897.910337306129</span></span><br><span class="line"><span class="number">699</span> <span class="number">602.2501347170239</span></span><br><span class="line"><span class="number">799</span> <span class="number">405.1310282453235</span></span><br><span class="line"><span class="number">899</span> <span class="number">273.64401431616227</span></span><br><span class="line"><span class="number">999</span> <span class="number">185.8900856790995</span></span><br><span class="line"><span class="number">1099</span> <span class="number">127.29091870514203</span></span><br><span class="line"><span class="number">1199</span> <span class="number">88.13752670192763</span></span><br><span class="line"><span class="number">1299</span> <span class="number">61.9609647056743</span></span><br><span class="line"><span class="number">1399</span> <span class="number">44.449054676909334</span></span><br><span class="line"><span class="number">1499</span> <span class="number">32.725892542610765</span></span><br><span class="line"><span class="number">1599</span> <span class="number">24.872466030243288</span></span><br><span class="line"><span class="number">1699</span> <span class="number">19.60757126015679</span></span><br><span class="line"><span class="number">1799</span> <span class="number">16.075338474872172</span></span><br><span class="line"><span class="number">1899</span> <span class="number">13.703685845905285</span></span><br><span class="line"><span class="number">1999</span> <span class="number">12.10998073226771</span></span><br><span class="line">Result: y = <span class="number">0.03244234711592508</span> + <span class="number">0.8095830142635627</span> x + -<span class="number">0.005596844720483275</span> x^<span class="number">2</span> + -<span class="number">0.08662259854633106</span> x^<span class="number">3</span></span><br></pre></td></tr></table></figure>

<p><img src="/%E6%89%BF/pytorch-samples/pytorch_warmup.png" alt="pytorch_warmup"></p>
<p>从图中可以看出，虽然存在一些误差，但是也大体描述了原函数图像</p>
<p>在这个例子中，我学习到了两点，一个是学习率，一个是梯度计算。</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><h3 id="深度学习-学习率-learning-rate"><a href="#深度学习-学习率-learning-rate" class="headerlink" title="深度学习: 学习率 (learning rate)"></a>深度学习: 学习率 (learning rate)</h3><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>学习率 (learning rate)，控制 模型的 <strong>学习进度</strong> ： </p>
<p><strong>lr</strong> 即 <strong>stride (步长)</strong> ，即反向传播算法中的 ηη ：</p>
<p>ωn←ωn−η∂L∂ωnωn←ωn−η∂L∂ωn</p>
<h4 id="学习率大小"><a href="#学习率大小" class="headerlink" title="学习率大小"></a>学习率大小</h4><table>
<thead>
<tr>
<th></th>
<th>学习率 大</th>
<th>学习率 小</th>
</tr>
</thead>
<tbody><tr>
<td>学习速度</td>
<td>快</td>
<td>慢</td>
</tr>
<tr>
<td>使用时间点</td>
<td>刚开始训练时</td>
<td>一定轮数过后</td>
</tr>
<tr>
<td>副作用</td>
<td>1，易损失值爆炸；2，易震荡</td>
<td>1，易过拟合；2，收敛速度慢</td>
</tr>
</tbody></table>
<h4 id="学习率设置"><a href="#学习率设置" class="headerlink" title="学习率设置"></a>学习率设置</h4><p>在训练过程中，一般根据<strong>训练轮数</strong>设置<strong>动态变化的学习率</strong>。</p>
<ul>
<li>刚开始训练时：学习率以 0.01 ~ 0.001 为宜。</li>
<li>一定轮数过后：逐渐减缓。</li>
<li>接近训练结束：学习速率的衰减应该在100倍以上。</li>
</ul>
<p><strong>Note：</strong><br>如果是 <strong>迁移学习</strong> ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤$10^{-4}$) 在新数据上进行 <strong>微调</strong> 。</p>
<h4 id="学习率减缓机制"><a href="#学习率减缓机制" class="headerlink" title="学习率减缓机制"></a>学习率减缓机制</h4><table>
<thead>
<tr>
<th></th>
<th>轮数减缓</th>
<th>指数减缓</th>
<th>分数减缓</th>
</tr>
</thead>
<tbody><tr>
<td>英文名</td>
<td>step decay</td>
<td>exponential decay</td>
<td>1/t1/t decay</td>
</tr>
<tr>
<td>方法</td>
<td>每N轮学习率减半</td>
<td>学习率按训练轮数增长指数插值递减</td>
<td>lrt=lr0/(1+kt)lrt=lr0/(1+kt) ，kk 控制减缓幅度，tt 为训练轮数</td>
</tr>
</tbody></table>
<h4 id="把脉-目标函数损失值-曲线"><a href="#把脉-目标函数损失值-曲线" class="headerlink" title="把脉 目标函数损失值 曲线"></a>把脉 目标函数损失值 曲线</h4><p>理想情况下 <strong>曲线</strong> 应该是 <strong>滑梯式下降</strong> [绿线]： </p>
<p><img src="/%E6%89%BF/pytorch-samples/20180808152358945" alt="img"></p>
<p>曲线 <strong>初始时 上扬</strong> [红线]： </p>
<p>Solution：<strong>初始</strong> 学习率过大 导致 <strong>振荡</strong>，应减小学习率，并 <strong>从头 开始训练</strong> 。</p>
<p>曲线 <strong>初始时 强势下降 没多久 归于水平</strong> [紫线]： </p>
<p>Solution：<strong>后期</strong> 学习率过大 导致 <strong>无法拟合</strong>，应减小学习率，并 <strong>重新训练 后几轮</strong> 。</p>
<p>曲线 <strong>全程缓慢</strong> [黄线]： </p>
<p>Solution：<strong>初始</strong> 学习率过小 导致 <strong>收敛慢</strong>，应增大学习率，并 <strong>从头 开始训练</strong> 。</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>根据以上学习方法，我绘制了这个程序损失函数的散点图，方便观察学习率是否合适</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">	print(t, loss)</span><br><span class="line">	plt.scatter(t,loss)	<span class="comment"># 根据损失值绘制散点图</span></span><br></pre></td></tr></table></figure>

<p><img src="/%E6%89%BF/pytorch-samples/pytorch_warmup_learningrate.png" alt="pytorch_warmup_learningrate"></p>
<p>图中纵轴是损失值，横轴是循环次数，可以看出这里的学习率还是比较符合要求。</p>
<h2 id="计算损失梯度"><a href="#计算损失梯度" class="headerlink" title="计算损失梯度"></a>计算损失梯度</h2><p>这里就是损失函数的反向传播计算了，大体来说就是根据损失函数的偏导数，计算梯度下降值。（我是这么理解的，如果有错误，请帮我指正）</p>
<p><strong>损失函数</strong>  $(y_{pred}-y)^2$</p>
<p>又有 $y_{pred}=a+bx+cx^2+dx^3$</p>
<p>根据链式求导原则，有<br>$$<br>\frac{\partial (y_{pred}-y)^2} {\partial a}=\frac{\partial (y_{pred}-y)^2} {\partial y_{pred}}\cdot\frac{\partial y_{pred}} {\partial a}<br>$$<br>计算出 $a=2(y_{pred}-y)$</p>
<p>同理可得 $b=2x(y_{pred}-y)\c=2x^2(y_{pred}-y)\d=2x^3(y_{pred}-y)$</p>
<p>至此，计算出了各个权值的偏导数</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/liulina603/article/details/80604385">深度学习: 学习率 (learning rate)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/alan-blog-TsingHua/p/9981522.html">[神经网络]反向传播梯度计算数学原理</a></p>
<hr>
<h1 id="PyTorch：张量"><a href="#PyTorch：张量" class="headerlink" title="PyTorch：张量"></a>PyTorch：张量</h1><p>经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测<code>y = sin(x)</code>从<code>-pi</code>到<code>pi</code>。</p>
<p>此实现使用 PyTorch 张量手动计算正向传播，损失和后向通过。</p>
<p>PyTorch 张量基本上与 numpy 数组相同：它对深度学习或计算图或梯度一无所知，只是用于任意数值计算的通用 n 维数组。</p>
<p>numpy 数组和 PyTorch 张量之间的最大区别是 PyTorch 张量可以在 CPU 或 GPU 上运行。 要在 GPU 上运行操作，只需将张量转换为 cuda 数据类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">dtype = torch.<span class="built_in">float</span></span><br><span class="line"><span class="comment"># device = torch.device(&quot;cpu&quot;)</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>) <span class="comment"># Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.linspace(-math.pi, math.pi, <span class="number">2000</span>, device=device, dtype=dtype)</span><br><span class="line">y = torch.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">a = torch.randn((), device=device, dtype=dtype)</span><br><span class="line">b = torch.randn((), device=device, dtype=dtype)</span><br><span class="line">c = torch.randn((), device=device, dtype=dtype)</span><br><span class="line">d = torch.randn((), device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    y_pred = a + b * x + c * x ** <span class="number">2</span> + d * x ** <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of a, b, c, d with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_a = grad_y_pred.<span class="built_in">sum</span>()</span><br><span class="line">    grad_b = (grad_y_pred * x).<span class="built_in">sum</span>()</span><br><span class="line">    grad_c = (grad_y_pred * x ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    grad_d = (grad_y_pred * x ** <span class="number">3</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    a -= learning_rate * grad_a</span><br><span class="line">    b -= learning_rate * grad_b</span><br><span class="line">    c -= learning_rate * grad_c</span><br><span class="line">    d -= learning_rate * grad_d</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;Result: y = <span class="subst">&#123;a.item()&#125;</span> + <span class="subst">&#123;b.item()&#125;</span> x + <span class="subst">&#123;c.item()&#125;</span> x^2 + <span class="subst">&#123;d.item()&#125;</span> x^3&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>本次实验内容与之前相同，只是利用了PyTorch，可以在GPU上运算，语法上有些许差别。</p>
<h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><h3 id="张量是什么？"><a href="#张量是什么？" class="headerlink" title="张量是什么？"></a>张量是什么？</h3><p>张量是一个多维数组，它是标量、向量、矩阵的高维拓展。</p>
<p><img src="/%E6%89%BF/pytorch-samples/20200701200703714.png" alt="在这里插入图片描述"></p>
<h3 id="张量的属性和性质"><a href="#张量的属性和性质" class="headerlink" title="张量的属性和性质"></a>张量的属性和性质</h3><ul>
<li>Variable</li>
</ul>
<p><img src="/%E6%89%BF/pytorch-samples/20200701200720530.png" alt="在这里插入图片描述"></p>
<ul>
<li>Tensor</li>
</ul>
<p>Tensor 增加 3 种属性</p>
<p><img src="/%E6%89%BF/pytorch-samples/20200701200736103.png" alt="在这里插入图片描述"></p>
<h3 id="Tensor-attributes"><a href="#Tensor-attributes" class="headerlink" title="Tensor attributes:"></a><strong>Tensor attributes:</strong></h3><p>在tensor attributes中有三个类，分别为torch.dtype, torch.device, 和 torch.layout</p>
<p>其中， torch.dtype 是展示 torch.Tensor 数据类型的类，pytorch 有八个不同的数据类型,下表是完整的 dtype 列表.</p>
<p><img src="/%E6%89%BF/pytorch-samples/v2-95729ebb10269f807b0809fb09b125d0_1440w.jpg" alt="img"></p>
<p>Torch.device 是表现 torch.Tensor被分配的设备类型的类，其中分为’cpu’ 和 ‘cuda’两种，如果设备序号没有显示则表示此 tensor 被分配到当前设备, 比如: ‘cuda’ 等同于 ‘cuda’: X , X 为torch.cuda.current _device() 返回值</p>
<p>我们可以通过 tensor.device 来获取其属性，同时可以利用字符或字符+序号的方式来分配设备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">通过字符串：</span><br><span class="line">&gt;&gt;&gt; torch.device(&#39;cuda:0&#39;)</span><br><span class="line">device(type&#x3D;&#39;cuda&#39;, index&#x3D;0)</span><br><span class="line">&gt;&gt;&gt; torch.device(&#39;cpu&#39;)</span><br><span class="line">device(type&#x3D;&#39;cpu&#39;)</span><br><span class="line">&gt;&gt;&gt; torch.device(&#39;cuda&#39;) # 当前设备</span><br><span class="line">device(type&#x3D;&#39;cuda&#39;)</span><br><span class="line"></span><br><span class="line">通过字符串和设备序号：</span><br><span class="line">&gt;&gt;&gt; torch.device(&#39;cuda&#39;, 0)</span><br><span class="line">device(type&#x3D;&#39;cuda&#39;, index&#x3D;0)</span><br><span class="line">&gt;&gt;&gt; torch.device(&#39;cpu&#39;, 0)</span><br><span class="line">device(type&#x3D;&#39;cpu&#39;, index&#x3D;0)</span><br></pre></td></tr></table></figure>

<p>此外，cpu 和 cuda 设备的转换使用 ‘to’ 来实现：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; device_cpu = torch.device(&quot;cuda&quot;)  #声明cuda设备</span><br><span class="line">&gt;&gt;&gt; device_cuda = torch.device(&#x27;cuda&#x27;)  #设备cpu设备</span><br><span class="line">&gt;&gt;&gt; data = torch.Tensor([1])</span><br><span class="line">&gt;&gt;&gt; data.to(device_cpu)  #将数据转为cpu格式</span><br><span class="line">&gt;&gt;&gt; data.to(device_cuda)   #将数据转为cuda格式</span><br></pre></td></tr></table></figure>



<p>torch.layout 是表现 torch.Tensor 内存分布的类，目前只支持 torch.strided</p>
<h3 id="创建tensor"><a href="#创建tensor" class="headerlink" title="创建tensor"></a><strong>创建tensor</strong></h3><ul>
<li>直接创建</li>
</ul>
<p>torch.tensor(data, dtype=None, device=None,requires_grad=False)</p>
<p>data - 可以是list, tuple, numpy array, scalar或其他类型</p>
<p>dtype - 可以返回想要的tensor类型</p>
<p>device - 可以指定返回的设备</p>
<p>requires_grad - 可以指定是否进行记录图的操作，默认为False</p>
<p>需要注意的是，torch.tensor 总是会复制 data, 如果你想避免复制，可以使 torch.Tensor. detach()，如果是从 numpy 中获得数据，那么你可以用 torch.from_numpy(), 注from_numpy() 是共享内存的</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])</span><br><span class="line">tensor([[ 0.1000,  1.2000],</span><br><span class="line">        [ 2.2000,  3.1000],</span><br><span class="line">        [ 4.9000,  5.2000]])</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data</span><br><span class="line">tensor([ 0,  1])</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],</span><br><span class="line">                 dtype=torch.float64,</span><br><span class="line">                 device=torch.device(&#x27;cuda:0&#x27;))  # creates a torch.cuda.DoubleTensor</span><br><span class="line">tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device=&#x27;cuda:0&#x27;)</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)</span><br><span class="line">tensor(3.1416)</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))</span><br><span class="line">tensor([])</span><br></pre></td></tr></table></figure>



<ul>
<li>从numpy中获得数据</li>
</ul>
<p>torch.from_numpy(ndarry)</p>
<p>注：生成返回的tensor会和ndarry共享数据，任何对tensor的操作都会影响到ndarry,<br>反之亦然</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = numpy.array([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; t = torch.from_numpy(a)</span><br><span class="line">&gt;&gt;&gt; t</span><br><span class="line">tensor([ 1,  2,  3])</span><br><span class="line">&gt;&gt;&gt; t[0] = -1</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">array([-1,  2,  3])</span><br></pre></td></tr></table></figure>



<ul>
<li>创建特定的tensor</li>
</ul>
<p>根据数值要求：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*sizes, out=None, ..)# 返回大小为sizes的零矩阵 </span><br><span class="line"></span><br><span class="line">torch.zeros_like(input, ..) # 返回与input相同size的零矩阵</span><br><span class="line"></span><br><span class="line">torch.ones(*sizes, out=None, ..) #f返回大小为sizes的单位矩阵</span><br><span class="line"></span><br><span class="line">torch.ones_like(input, ..) #返回与input相同size的单位矩阵</span><br><span class="line"></span><br><span class="line">torch.full(size, fill_value, …) #返回大小为sizes,单位值为fill_value的矩阵</span><br><span class="line"></span><br><span class="line">torch.full_like(input, fill_value, …) 返回与input相同size，单位值为fill_value的矩阵</span><br><span class="line"></span><br><span class="line">torch.arange(start=0, end, step=1, …) #返回从start到end, 单位步长为step的1-d tensor.</span><br><span class="line"></span><br><span class="line">torch.linspace(start, end, steps=100, …)  #返回从start到end, 间隔中的插值数目为steps的1-d tensor</span><br><span class="line"></span><br><span class="line">torch.logspace(start, end, steps=100, …) #返回1-d tensor ，从10^start到10^end的steps个对数间隔</span><br></pre></td></tr></table></figure>

<p>根据矩阵要求:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(n, m=None, out=None,…) #返回2-D 的单位对角矩阵</span><br><span class="line"></span><br><span class="line">torch.empty(*sizes, out=None, …) #返回被未初始化的数值填充，大小为sizes的tensor</span><br><span class="line"></span><br><span class="line">torch.empty_like(input, …) # 返回与input相同size,并被未初始化的数值填充的tensor</span><br></pre></td></tr></table></figure>



<ul>
<li><em>随机采用生成:</em></li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean, std, out=None)</span><br><span class="line"></span><br><span class="line">torch.rand(*size, out=None, dtype=None, …) #返回[0,1]之间均匀分布的随机数值</span><br><span class="line"></span><br><span class="line">torch.rand_like(input, dtype=None, …) #返回与input相同size的tensor, 填充均匀分布的随机数值</span><br><span class="line"></span><br><span class="line">torch.randint(low=0, high, size,…) #返回均匀分布的[low,high]之间的整数随机值</span><br><span class="line"></span><br><span class="line">torch.randint_like(input, low=0, high, dtype=None, …) #</span><br><span class="line"></span><br><span class="line">torch.randn(*sizes, out=None, …) #返回大小为size,由均值为0，方差为1的正态分布的随机数值</span><br><span class="line"></span><br><span class="line">torch.randn_like(input, dtype=None, …)</span><br><span class="line"></span><br><span class="line">torch.randperm(n, out=None, dtype=torch.int64) # 返回0到n-1的数列的随机排列</span><br></pre></td></tr></table></figure>







<h3 id="操作tensor"><a href="#操作tensor" class="headerlink" title="操作tensor"></a><strong>操作tensor</strong></h3><p>基本操作：</p>
<p>Joining ops:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(seq,dim=0,out=None) # 沿着dim连接seq中的tensor, 所有的tensor必须有相同的size或为empty， 其相反的操作为 torch.split() 和torch.chunk()</span><br><span class="line">torch.stack(seq, dim=0, out=None) #同上</span><br><span class="line"></span><br><span class="line">#注: .cat 和 .stack的区别在于 cat会增加现有维度的值,可以理解为续接，stack会新加增加一个维度，可以</span><br><span class="line">理解为叠加</span><br><span class="line">&gt;&gt;&gt; a=torch.Tensor([1,2,3])</span><br><span class="line">&gt;&gt;&gt; torch.stack((a,a)).size()</span><br><span class="line">torch.size(2,3)</span><br><span class="line">&gt;&gt;&gt; torch.cat((a,a)).size()</span><br><span class="line">torch.size(6)</span><br></pre></td></tr></table></figure>



<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.gather(input, dim, index, out=None) #返回沿着dim收集的新的tensor</span><br><span class="line">&gt;&gt; t = torch.Tensor([[1,2],[3,4]])</span><br><span class="line">&gt;&gt; index = torch.LongTensor([[0,0],[1,0]])</span><br><span class="line">&gt;&gt; torch.gather(t, 0, index) #由于 dim=0,所以结果为</span><br><span class="line">| t[index[0, 0] 0]   t[index[0, 1] 1] |</span><br><span class="line">| t[index[1, 0] 0]   t[index[1, 1] 1] |</span><br><span class="line"></span><br><span class="line">对于3-D 的张量来说，可以作为</span><br><span class="line"></span><br><span class="line">out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0</span><br><span class="line">out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1</span><br><span class="line">out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</span><br></pre></td></tr></table></figure>



<p>clicing ops:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.split(tensor, split_size_or_sections, dim=0) #将tensor 拆分成相应的组块</span><br><span class="line">torch.chunk(tensor, chunks, dim=0) #将tensor 拆分成相应的组块， 最后一块会小一些如果不能整除的话#</span><br><span class="line"></span><br><span class="line">#注：split和chunk的区别在于：</span><br><span class="line">split的split_size_or_sections 表示每一个组块中的数据大小，chunks表示组块的数量</span><br><span class="line">&gt;&gt;&gt; a = torch.Tensor([1,2,3])</span><br><span class="line">&gt;&gt;&gt; torch.split(a,1)</span><br><span class="line">(tensor([1.]), tensor([2.]), tensor([3.]))</span><br><span class="line">&gt;&gt;&gt; torch.chunk(a,1)</span><br><span class="line">(tensor([ 1., 2., 3.]),)</span><br></pre></td></tr></table></figure>



<p>Indexing ops:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(input, dim, index, out=None) #返回沿着dim的指定tensor, index需为longTensor类型，不共用内存</span><br><span class="line"></span><br><span class="line">torch.masked_select(input, mask, out=None) #根据mask来返回input的值其为1-D tensor. Mask为ByteTensor, true返回，false不返回，返回值不共用内存</span><br><span class="line">&gt;&gt;&gt; x = torch.randn(3, 4)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],</span><br><span class="line">        [-1.2035,  1.2252,  0.5002,  0.6248],</span><br><span class="line">        [ 0.1307, -2.0608,  0.1244,  2.0139]])</span><br><span class="line">&gt;&gt;&gt; mask = x.ge(0.5)</span><br><span class="line">&gt;&gt;&gt; mask</span><br><span class="line">tensor([[ 0,  0,  0,  0],</span><br><span class="line">        [ 0,  1,  1,  1],</span><br><span class="line">        [ 0,  0,  0,  1]], dtype=torch.uint8)</span><br><span class="line">&gt;&gt;&gt; torch.masked_select(x, mask)</span><br><span class="line">tensor([ 1.2252,  0.5002,  0.6248,  2.0139])</span><br></pre></td></tr></table></figure>





<p>Mutation ops:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">torch.transpose(input, dim0, dim1, out=None) #返回dim0和dim1交换后的tensor</span><br><span class="line">torch.t(input, out=None) #专为2D矩阵的转置，是transpose的便捷函数</span><br><span class="line"></span><br><span class="line">torch.squeeze(input, dim, out=None)  #默认移除所有size为1的维度，当dim指定时，移除指定size为1的维度. 返回的tensor会和input共享存储空间，所以任何一个的改变都会影响另一个</span><br><span class="line">torch.unsqueeze(input, dim, out=None) #扩展input的size, 如 A x B 变为 1 x A x B </span><br><span class="line"></span><br><span class="line">torch.reshape(input, shape) #返回size为shape具有相同数值的tensor, 注意 shape=(-1,)这种表述，-1表示任意的。</span><br><span class="line">#注 reshape(-1,)</span><br><span class="line">&gt;&gt;&gt; a=torch.Tensor([1,2,3,4,5]) #a.size 是 torch.size(5)</span><br><span class="line">&gt;&gt;&gt; b=a.reshape(1,-1)  #表示第一维度是1，第二维度按a的size填充满</span><br><span class="line">&gt;&gt;&gt; b.size()</span><br><span class="line">torch.size([1,5])</span><br><span class="line"></span><br><span class="line">torch.where(condition,x,y) #根据condition的值来相应x,y的值，true返回x的值，false返回y的值，形成新的tensor</span><br><span class="line"></span><br><span class="line">torch.unbind(tensor, dim=0) #返回tuple 解除指定的dim的绑定,相当于按指定dim拆分</span><br><span class="line">&gt;&gt;&gt; a=torch.Tensor([[1,2,3],[2,3,4]])</span><br><span class="line">&gt;&gt;&gt; torch.unbind(a,dim=0)</span><br><span class="line">(torch([1,2,3]),torch([2,3,4])) # 将一个(2,3) 分为两个(3)</span><br><span class="line"></span><br><span class="line">torch.nonzero(input, out=None) # 返回非零值的索引， 每一行都是一个非零值的索引值</span><br><span class="line">&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))</span><br><span class="line">tensor([[ 0],</span><br><span class="line">        [ 1],</span><br><span class="line">        [ 2],</span><br><span class="line">        [ 4]])</span><br><span class="line">&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],</span><br><span class="line">                                [0.0, 0.4, 0.0, 0.0],</span><br><span class="line">                                [0.0, 0.0, 1.2, 0.0],</span><br><span class="line">                                [0.0, 0.0, 0.0,-0.4]]))</span><br><span class="line">tensor([[ 0,  0],</span><br><span class="line">        [ 1,  1],</span><br><span class="line">        [ 2,  2],</span><br><span class="line">        [ 3,  3]])</span><br></pre></td></tr></table></figure>



<h3 id="Tensor操作"><a href="#Tensor操作" class="headerlink" title="Tensor操作"></a><strong>Tensor操作</strong></h3><ul>
<li>点对点操作</li>
</ul>
<p>三角函数：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.abs(input, out=None)</span><br><span class="line">torch.acos(input, out=None)</span><br><span class="line">torch.asin(input, out=None)</span><br><span class="line">torch.atan(input, out=None)</span><br><span class="line">torch.atan2(input, inpu2, out=None) </span><br><span class="line">torch.cos(input, out=None)</span><br><span class="line">torch.cosh(input, out=None)</span><br><span class="line">torch.sin(input, out=None)</span><br><span class="line">torch.sinh(input, out=None)</span><br><span class="line">torch.tan(input, out=None)</span><br><span class="line">torch.tanh(input, out=None)</span><br></pre></td></tr></table></figure>



<p>基本运算，加减乘除</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Torch.add(input, value, out=None)</span><br><span class="line">          .add(input, value=1, other, out=None)</span><br><span class="line">          .addcdiv(tensor, value=1, tensor1, tensor2, out=None)</span><br><span class="line">          .addcmul(tensor, value=1, tensor1, tensor2, out=None)</span><br><span class="line">torch.div(input, value, out=None)</span><br><span class="line">         .div(input, other, out=None)</span><br><span class="line">torch.mul(input, value, out=None)</span><br><span class="line">        .mul(input, other, out=None)</span><br></pre></td></tr></table></figure>



<p>对数运算：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.log(input, out=None)  # y_i=log_e(x_i)</span><br><span class="line">torch.log1p(input, out=None)  #y_i=log_e(x_i+1)</span><br><span class="line">torch.log2(input, out=None)   #y_i=log_2(x_i)</span><br><span class="line">torch.log10(input,out=None)  #y_i=log_10(x_i)</span><br></pre></td></tr></table></figure>



<p>幂函数：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.pow(input, exponent, out=None)  # y_i=input^(exponent)</span><br></pre></td></tr></table></figure>



<p>指数运算</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(tensor, out=None)    #y_i=e^(x_i)</span><br><span class="line">torch.expm1(tensor, out=None)   #y_i=e^(x_i) -1</span><br></pre></td></tr></table></figure>



<p>截断函数</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.ceil(input, out=None)   #返回向正方向取得最小整数</span><br><span class="line">torch.floor(input, out=None)  #返回向负方向取得最大整数</span><br><span class="line"></span><br><span class="line">torch.round(input, out=None)  #返回相邻最近的整数，四舍五入</span><br><span class="line"></span><br><span class="line">torch.trunc(input, out=None)  #返回整数部分数值</span><br><span class="line">torch.frac(tensor, out=None)  #返回小数部分数值</span><br><span class="line"></span><br><span class="line">torch.fmod(input, divisor, out=None)  #返回input/divisor的余数</span><br><span class="line">torch.remainder(input, divisor, out=None)  #同上</span><br></pre></td></tr></table></figure>



<p>其他运算</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">torch.erf(tensor， out=None)</span><br><span class="line"> </span><br><span class="line">torch.erfinv(tensor, out=None)</span><br><span class="line"> </span><br><span class="line">torch.sigmoid(input, out=None)</span><br><span class="line"> </span><br><span class="line">torch.clamp(input, min, max out=None)  #返回 input&lt;min,则返回min, input&gt;max,则返回max,其余返回input</span><br><span class="line"></span><br><span class="line">torch.neg(input, out=None) #out_i=-1*(input)</span><br><span class="line"></span><br><span class="line">torch.reciprocal(input, out=None)  # out_i= 1/input_i</span><br><span class="line"></span><br><span class="line">torch.sqrt(input, out=None)  # out_i=sqrt(input_i)</span><br><span class="line">torch.rsqrt(input, out=None) #out_i=1/(sqrt(input_i))</span><br><span class="line"></span><br><span class="line">torch.sign(input, out=None)  #out_i=sin(input_i)  大于0为1，小于0为-1</span><br><span class="line"></span><br><span class="line">torch.lerp(start, end, weight, out=None)</span><br></pre></td></tr></table></figure>



<ul>
<li>降维操作</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">torch.argmax(input, dim=None, keepdim=False) #返回最大值排序的索引值</span><br><span class="line">torch.argmin(input, dim=None, keepdim=False)  #返回最小值排序的索引值</span><br><span class="line"></span><br><span class="line">torch.cumprod(input, dim, out=None)  #y_i=x_1 * x_2 * x_3 *…* x_i</span><br><span class="line">torch.cumsum(input, dim, out=None)  #y_i=x_1 + x_2 + … + x_i</span><br><span class="line"></span><br><span class="line">torch.dist(input, out, p=2)       #返回input和out的p式距离</span><br><span class="line">torch.mean()                      #返回平均值</span><br><span class="line">torch.sum()                       #返回总和</span><br><span class="line">torch.median(input)               #返回中间值</span><br><span class="line">torch.mode(input)                 #返回众数值</span><br><span class="line">torch.unique(input, sorted=False) #返回1-D的唯一的tensor,每个数值返回一次.</span><br><span class="line">&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))</span><br><span class="line">&gt;&gt;&gt; output</span><br><span class="line">tensor([ 2,  3,  1])</span><br><span class="line"></span><br><span class="line">torch.std(  #返回标准差)</span><br><span class="line">torch.var() #返回方差</span><br><span class="line"></span><br><span class="line">torch.norm(input, p=2) #返回p-norm的范式</span><br><span class="line">torch.prod(input, dim, keepdim=False) #返回指定维度每一行的乘积</span><br></pre></td></tr></table></figure>



<ul>
<li>对比操作：</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(input, other, out=None)  #按成员进行等式操作，相同返回1</span><br><span class="line">torch.equal(tensor1, tensor2) #如果tensor1和tensor2有相同的size和elements，则为true</span><br><span class="line">&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span><br><span class="line">tensor([[ 1,  0],</span><br><span class="line">        [ 0,  1]], dtype=torch.uint8)</span><br><span class="line">&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span><br><span class="line">tensor([[ 1,  0],</span><br><span class="line">        [ 0,  1]], dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">torch.ge(input, other, out=None)   # input&gt;= other</span><br><span class="line">torch.gt(input, other, out=None)   # input&gt;other</span><br><span class="line">torch.le(input, other, out=None)    # input=&lt;other</span><br><span class="line">torch.lt(input, other, out=None)    # input&lt;other</span><br><span class="line">torch.ne(input, other, out=None)  # input != other 不等于</span><br><span class="line"></span><br><span class="line">torch.max()                        # 返回最大值</span><br><span class="line">torch.min()                        # 返回最小值</span><br><span class="line">torch.isnan(tensor) #判断是否为’nan’</span><br><span class="line">torch.sort(input, dim=None, descending=False, out=None) #对目标input进行排序</span><br><span class="line">torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)  #沿着指定维度返回最大k个数值及其索引值</span><br><span class="line">torch.kthvalue(input, k, dim=None, deepdim=False, out=None) #沿着指定维度返回最小k个数值及其索引值</span><br></pre></td></tr></table></figure>



<ul>
<li>频谱操作</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.fft(input, signal_ndim, normalized=False)</span><br><span class="line">torch.ifft(input, signal_ndim, normalized=False)</span><br><span class="line">torch.rfft(input, signal_ndim, normalized=False, onesided=True)</span><br><span class="line">torch.irfft(input, signal_ndim, normalized=False, onesided=True)</span><br><span class="line">torch.stft(signa, frame_length, hop, …)</span><br></pre></td></tr></table></figure>



<ul>
<li>其他操作：</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">torch.cross(input, other, dim=-1, out=None)  #叉乘(外积)</span><br><span class="line"></span><br><span class="line">torch.dot(tensor1, tensor2)  #返回tensor1和tensor2的点乘</span><br><span class="line"></span><br><span class="line">torch.mm(mat1, mat2, out=None) #返回矩阵mat1和mat2的乘积</span><br><span class="line"></span><br><span class="line">torch.eig(a, eigenvectors=False, out=None) #返回矩阵a的特征值/特征向量 </span><br><span class="line"></span><br><span class="line">torch.det(A)  #返回矩阵A的行列式</span><br><span class="line"></span><br><span class="line">torch.trace(input) #返回2-d 矩阵的迹(对对角元素求和)</span><br><span class="line"></span><br><span class="line">torch.diag(input, diagonal=0, out=None) #</span><br><span class="line"></span><br><span class="line">torch.histc(input, bins=100, min=0, max=0, out=None) #计算input的直方图</span><br><span class="line"></span><br><span class="line">torch.tril(input, diagonal=0, out=None)  #返回矩阵的下三角矩阵，其他为0</span><br><span class="line"></span><br><span class="line">torch.triu(input, diagonal=0, out=None) #返回矩阵的上三角矩阵，其他为0</span><br></pre></td></tr></table></figure>



<h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips:"></a>Tips:</h3><ul>
<li>获取python number:</li>
</ul>
<p>由于pytorch 0.4后，python number的获取统一通过 .item()方式实现：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.Tensor([1,2,3])</span><br><span class="line">&gt;&gt;&gt; a[0]   #直接取索引返回的是tensor数据</span><br><span class="line">tensor(1.)</span><br><span class="line">&gt;&gt;&gt; a[0].item()  #获取python number</span><br><span class="line">1</span><br></pre></td></tr></table></figure>



<ul>
<li>tensor设置</li>
</ul>
<p>判断:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.is_tensor()  #如果是pytorch的tensor类型返回true</span><br><span class="line">torch.is_storage() # 如果是pytorch的storage类型返回ture</span><br></pre></td></tr></table></figure>



<p>这里还有一个小技巧，如果需要判断tensor是否为空，可以如下</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a=torch.Tensor()</span><br><span class="line">&gt;&gt;&gt; len(a)</span><br><span class="line">0</span><br><span class="line">&gt;&gt;&gt; len(a) is 0</span><br><span class="line">True</span><br></pre></td></tr></table></figure>



<p>设置: 通过一些内置函数，可以实现对tensor的精度, 类型，print打印参数等进行设置</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">torch.set_default_dtype(d)  #对torch.tensor() 设置默认的浮点类型</span><br><span class="line"></span><br><span class="line">torch.set_default_tensor_type() # 同上，对torch.tensor()设置默认的tensor类型</span><br><span class="line">&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # initial default for floating point is torch.float32</span><br><span class="line">torch.float32</span><br><span class="line">&gt;&gt;&gt; torch.set_default_dtype(torch.float64)</span><br><span class="line">&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # a new floating point tensor</span><br><span class="line">torch.float64</span><br><span class="line">&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line">&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor</span><br><span class="line">torch.float64</span><br><span class="line"></span><br><span class="line">torch.get_default_dtype() #获得当前默认的浮点类型torch.dtype</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None）#)</span><br><span class="line">## 设置printing的打印参数</span><br></pre></td></tr></table></figure>





<h3 id="张量的创建"><a href="#张量的创建" class="headerlink" title="张量的创建"></a>张量的创建</h3><h4 id="1-直接创建"><a href="#1-直接创建" class="headerlink" title="1. 直接创建"></a>1. 直接创建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(</span><br><span class="line">            data, <span class="comment"># 数据, 可以是list, numpy</span></span><br><span class="line">            dtype=<span class="literal">None</span>, <span class="comment"># 数据类型，默认与data的一致</span></span><br><span class="line">            device=<span class="literal">None</span>,</span><br><span class="line">            requires_grad=<span class="literal">False</span>,</span><br><span class="line">            pin_memory=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">torch.from_numpy() <span class="comment"># tensor 与原 ndarray 共享内存</span></span><br></pre></td></tr></table></figure>

<h4 id="2-依据数值创建"><a href="#2-依据数值创建" class="headerlink" title="2. 依据数值创建"></a>2. 依据数值创建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 依size创建全0张量</span></span><br><span class="line">torch.zeros(*size, <span class="comment"># size: 张量的形状, 如(3, 3)、 (3, 224,224)</span></span><br><span class="line">            out=<span class="literal">None</span>, <span class="comment"># 输出的张量，和返回的张量指向同一内存地址</span></span><br><span class="line">            dtype=<span class="literal">None</span>,</span><br><span class="line">            layout=torch.strided, <span class="comment"># 内存中布局形式, 有strided,sparse_coo等</span></span><br><span class="line">            device=<span class="literal">None</span>,</span><br><span class="line">            requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 依input形状创建全0张量</span></span><br><span class="line">torch.zeros_like(<span class="built_in">input</span>, <span class="comment"># 创建与input同形状的全0张量</span></span><br><span class="line">                dtype=<span class="literal">None</span>,</span><br><span class="line">                layout=<span class="literal">None</span>,</span><br><span class="line">                device=<span class="literal">None</span>,</span><br><span class="line">                requires_grad=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 全1张量，参数同上</span></span><br><span class="line">torch.ones()       </span><br><span class="line">torch.ones_like()</span><br><span class="line">           </span><br><span class="line"><span class="comment"># 依input形状创建指定数据的张量(剩余参数隐藏)</span></span><br><span class="line"><span class="comment"># t = torch.full((3, 3), 1)</span></span><br><span class="line">torch.full(size, <span class="comment"># 张量的形状, 如(3, 3)</span></span><br><span class="line">            fill_value, <span class="comment"># 张量的值</span></span><br><span class="line">          )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建等差的1维张量</span></span><br><span class="line">torch.arange(start=<span class="number">0</span>,</span><br><span class="line">             end,</span><br><span class="line">             step=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 创建均分的1维张量</span></span><br><span class="line">torch.linspace(start,</span><br><span class="line">               end,</span><br><span class="line">               steps=<span class="number">100</span>)	<span class="comment"># 数列长度</span></span><br><span class="line"><span class="comment"># 创建对数均分的1维张量</span></span><br><span class="line">torch.logspace(start,</span><br><span class="line">               end,</span><br><span class="line">               steps=<span class="number">100</span>, <span class="comment"># 数列长度</span></span><br><span class="line">               base=<span class="number">10.0</span>) <span class="comment"># 对数函数的底，默认为10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建单位对角矩阵（2维张量）</span></span><br><span class="line">torch.eye(n, <span class="comment"># 矩阵行数</span></span><br><span class="line">          m=<span class="literal">None</span>, <span class="comment"># 矩阵列数</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>

<h4 id="3-依概率分布创建张量"><a href="#3-依概率分布创建张量" class="headerlink" title="3. 依概率分布创建张量"></a>3. 依概率分布创建张量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成正态分布（高斯分布）</span></span><br><span class="line">torch.normal(mean,</span><br><span class="line">            std,</span><br><span class="line">            out=<span class="literal">None</span>)</span><br><span class="line">四种模式：</span><br><span class="line">mean为标量， std为标量</span><br><span class="line">mean为标量， std为张量</span><br><span class="line">mean为张量， std为标量</span><br><span class="line">mean为张量， std为张量</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成标准正态分布</span></span><br><span class="line">torch.randn(*size) <span class="comment"># 张量的形状</span></span><br><span class="line">torch.randn_like()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在区间[0, 1)上，生成均匀分布</span></span><br><span class="line">torch.rand(*size)</span><br><span class="line">torch.rand_like()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 区间[low, high)生成整数均匀分布</span></span><br><span class="line">torch.randint()</span><br><span class="line">torch.randint_like()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成生成从0到n-1的随机排列</span></span><br><span class="line">torch.randperm(n)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以input为概率，生成伯努力分布（ 0-1分布，两点分布）</span></span><br><span class="line">torch.bernoulli()</span><br></pre></td></tr></table></figure>

<h3 id="张量的操作：拼接、切分、索引和变换"><a href="#张量的操作：拼接、切分、索引和变换" class="headerlink" title="张量的操作：拼接、切分、索引和变换"></a>张量的操作：拼接、切分、索引和变换</h3><h4 id="张量拼接与切分"><a href="#张量拼接与切分" class="headerlink" title="张量拼接与切分"></a>张量拼接与切分</h4><p>拼接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将张量按维度dim进行拼接</span></span><br><span class="line">torch.cat(tensors, <span class="comment">#　张量序列</span></span><br><span class="line">        dim=<span class="number">0</span>,　<span class="comment"># 要拼接的维度</span></span><br><span class="line">        out=<span class="literal">None</span>)</span><br><span class="line">torch.cat([torch.ones((<span class="number">2</span>, <span class="number">3</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>))], dim=<span class="number">0</span>) <span class="comment"># 在第 0 维拼接，shape(4, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在新创建的维度dim上   进行拼接</span></span><br><span class="line">torch.stack(tensors, <span class="comment"># 张量序列</span></span><br><span class="line">            dim=<span class="number">0</span>, <span class="comment"># 要拼接的维度</span></span><br><span class="line">            out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>切分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将张量按维度dim进行平均切分；返回值：张量列表</span></span><br><span class="line">torch.chunk(<span class="built_in">input</span>, <span class="comment"># 要切分的张量</span></span><br><span class="line">            chunks, <span class="comment"># 要切分的份数</span></span><br><span class="line">            dim=<span class="number">0</span>) <span class="comment">#  要切分的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将张量按维度dim进行切分；返回值：张量列表</span></span><br><span class="line">torch.split(tensor, <span class="comment"># 要切分的张量</span></span><br><span class="line">            split_size_or_sections, <span class="comment"># 为int时，表示每一份的长度；为list时，按list元素切分</span></span><br><span class="line">            dim=<span class="number">0</span>) <span class="comment"># 要切分的维度</span></span><br></pre></td></tr></table></figure>

<h4 id="张量索引与变换"><a href="#张量索引与变换" class="headerlink" title="张量索引与变换"></a>张量索引与变换</h4><p>索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在维度dim上，按index索引数据；返回值：依index索引数据拼接的张量</span></span><br><span class="line">torch.index_select(<span class="built_in">input</span>, <span class="comment"># 要索引的张量</span></span><br><span class="line">                    dim, <span class="comment"># 要索引的维度</span></span><br><span class="line">                    index, <span class="comment"># 要索引数据的序号 torch.tensor([0, 2], dtype=torch.long)</span></span><br><span class="line">                    out=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按mask中的True进行索引；返回值：一维张量</span></span><br><span class="line">torch.masked_select(<span class="built_in">input</span>, <span class="comment"># 要索引的张量</span></span><br><span class="line">                    mask, <span class="comment"># 与input同形状的布尔类型张量</span></span><br><span class="line">                    out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>变换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变换张量形状（注意事项：当张量在内存中是连续时，新张量与input共享数据内存）</span></span><br><span class="line">torch.reshape(<span class="built_in">input</span>, </span><br><span class="line">              shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交换张量的两个维度</span></span><br><span class="line">torch.transpose(<span class="built_in">input</span>, <span class="comment"># 要变换的张量</span></span><br><span class="line">                dim0, <span class="comment"># 要交换的维度</span></span><br><span class="line">                dim1) <span class="comment"># 要交换的维度</span></span><br><span class="line"><span class="comment"># 2维张量转置</span></span><br><span class="line">torch.t(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 压缩长度为1的维度（轴）</span></span><br><span class="line">torch.squeeze(<span class="built_in">input</span>,</span><br><span class="line">            dim=<span class="literal">None</span>, <span class="comment"># 若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除；</span></span><br><span class="line">            out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h3 id="张量数学运算"><a href="#张量数学运算" class="headerlink" title="张量数学运算"></a>张量数学运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加减乘除</span></span><br><span class="line">torch.add()</span><br><span class="line">torch.addcdiv()</span><br><span class="line">torch.addcmul()</span><br><span class="line">torch.sub()</span><br><span class="line">torch.div()</span><br><span class="line">torch.mul()</span><br><span class="line"><span class="comment"># 对数，指数，幂函数</span></span><br><span class="line">torch.log(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.log10(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.log2(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.exp(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.<span class="built_in">pow</span>()</span><br><span class="line"><span class="comment"># 三角函数</span></span><br><span class="line">torch.<span class="built_in">abs</span>(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.acos(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.cosh(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.cos(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.asin(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.atan(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.atan2(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逐元素计算 </span></span><br><span class="line"><span class="comment"># input + alpha × other</span></span><br><span class="line">torch.add(<span class="built_in">input</span>, <span class="comment"># 第一个张量</span></span><br><span class="line">          alpha=<span class="number">1</span>, <span class="comment"># 乘项因子</span></span><br><span class="line">          other, <span class="comment"># 第二个张量</span></span><br><span class="line">          out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>



<h1 id="PyTorch：张量和-Autograd"><a href="#PyTorch：张量和-Autograd" class="headerlink" title="PyTorch：张量和 Autograd"></a>PyTorch：张量和 Autograd</h1><p>经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测<code>y = sin(x)</code>从<code>-pi</code>到<code>pi</code>。</p>
<p>此实现使用 PyTorch 张量上的运算来计算正向传播，并使用 PyTorch Autograd 来计算梯度。</p>
<p>PyTorch 张量表示计算图中的一个节点。 如果<code>x</code>是具有<code>x.requires_grad=True</code>的张量，则<code>x.grad</code>是另一个张量，其保持<code>x</code>相对于某个标量值的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">dtype = torch.<span class="built_in">float</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># device = torch.device(&quot;cuda:0&quot;)  # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Tensors to hold input and outputs. 创建张量来保存输入和输出。</span></span><br><span class="line"><span class="comment"># By default, requires_grad=False, which indicates that we do not need to</span></span><br><span class="line"><span class="comment"># compute gradients with respect to these Tensors during the backward pass.</span></span><br><span class="line"><span class="comment"># 默认情况下，requires_grad=False，这表示我们不需要在向后传递期间计算关于这些张量的梯度。</span></span><br><span class="line">x = torch.linspace(-math.pi, math.pi, <span class="number">2000</span>, device=device, dtype=dtype)</span><br><span class="line">y = torch.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights. For a third order polynomial, we need</span></span><br><span class="line"><span class="comment"># 4 weights: y = a + b x + c x^2 + d x^3</span></span><br><span class="line"><span class="comment"># 为权重创建随机张量。对于三阶多项式，我们需要4个权值：y=a+bx+cx^2+dx^3</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Tensors during the backward pass.</span></span><br><span class="line"><span class="comment"># 设置requires_grad=True表示我们希望在向后传递期间计算相对于这些张量的梯度。</span></span><br><span class="line">a = torch.randn((), device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn((), device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = torch.randn((), device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = torch.randn((), device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Tensors. 前向传递：使用张量运算计算预测的y。</span></span><br><span class="line">    y_pred = a + b * x + c * x ** <span class="number">2</span> + d * x ** <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss using operations on Tensors. 使用张量运算计算并打印损耗。</span></span><br><span class="line">    <span class="comment"># Now loss is a Tensor of shape (1,) 现在损失是一个形状的张量（1，）</span></span><br><span class="line">    <span class="comment"># loss.item() gets the scalar value held in the loss. loss.item()获取丢失中保存的标量值。</span></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></span><br><span class="line">    <span class="comment"># gradient of loss with respect to all Tensors with requires_grad=True.</span></span><br><span class="line">    <span class="comment"># 使用autograd计算向后传球。此调用将计算与所有张量相关的损失梯度，且requires_grad=True。</span></span><br><span class="line">    <span class="comment"># After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding</span></span><br><span class="line">    <span class="comment"># the gradient of the loss with respect to a, b, c, d respectively.</span></span><br><span class="line">    <span class="comment"># 在这之后调用a.grad，b.grad c.grad和d.grad将分别是关于a，b，c，d保持损失梯度的张量。</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually update weights using gradient descent. Wrap in torch.no_grad()</span></span><br><span class="line">    <span class="comment"># because weights have requires_grad=True, but we don&#x27;t need to track this</span></span><br><span class="line">    <span class="comment"># in autograd.</span></span><br><span class="line">    <span class="comment"># 使用梯度下降手动更新权重。包裹火炬号（）因为权重要求_grad=True，但我们不需要在autograd中跟踪它。</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        a -= learning_rate * a.grad</span><br><span class="line">        b -= learning_rate * b.grad</span><br><span class="line">        c -= learning_rate * c.grad</span><br><span class="line">        d -= learning_rate * d.grad</span><br><span class="line">        </span><br><span class="line">        print(a.grad)</span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights 更新权重后手动将渐变归零</span></span><br><span class="line">        a.grad = <span class="literal">None</span></span><br><span class="line">        b.grad = <span class="literal">None</span></span><br><span class="line">        c.grad = <span class="literal">None</span></span><br><span class="line">        d.grad = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;Result: y = <span class="subst">&#123;a.item()&#125;</span> + <span class="subst">&#123;b.item()&#125;</span> x + <span class="subst">&#123;c.item()&#125;</span> x^2 + <span class="subst">&#123;d.item()&#125;</span> x^3&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>首先，我们先简单地介绍一下什么是计算图（Computational Graphs），以方便后边的讲解。假设我们有一个复杂的神经网络模型，我们把它想象成一个错综复杂的管道结构，不同的管道之间通过节点连接起来，我们有一个注水口，一个出水口。我们在入口注入数据的之后，数据就沿着设定好的管道路线缓缓流动到出水口，这时候我们就完成了一次正向传播。想象一下输入的 tensor 数据在管道中缓缓流动的场景，这就是为什么 TensorFlow 叫 Tensor<strong>Flow</strong> 的原因！emmm，好像走错片场了，不过计算图在 PyTorch 中也是类似的。至于这两个非常有代表性的深度学习框架在计算图上有什么区别，我们稍后再谈。</p>
<p>计算图通常包含两种元素，一个是 tensor，另一个是 Function。张量 tensor 不必多说，但是大家可能对 Function 比较陌生。这里 Function 指的是在计算图中某个节点（node）所进行的运算，比如加减乘除卷积等等之类的，Function 内部有 <code>forward()</code> 和 <code>backward()</code> 两个方法，分别应用于正向、反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a.exp()</span><br><span class="line">print(b)</span><br><span class="line"><span class="comment"># tensor(7.3891, grad_fn=&lt;ExpBackward&gt;)</span></span><br></pre></td></tr></table></figure>

<p>在我们做正向传播的过程中，除了执行 <code>forward()</code> 操作之外，还会同时会为反向传播做一些准备，为反向计算图添加 Function 节点。在上边这个例子中，变量 <code>b</code> 在反向传播中所需要进行的操作是 <code>&lt;ExpBackward&gt;</code> 。</p>
<h3 id="一个具体的例子"><a href="#一个具体的例子" class="headerlink" title="一个具体的例子"></a>一个具体的例子</h3><p>了解了基础知识之后，现在我们来看一个具体的计算例子，并画出它的正向和反向计算图。假如我们需要计算这么一个模型：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1 = input x w1</span><br><span class="line">l2 = l1 + w2</span><br><span class="line">l3 = l1 x w3</span><br><span class="line">l4 = l2 x l3</span><br><span class="line">loss = mean(l4)</span><br></pre></td></tr></table></figure>

<p>这个例子比较简单，涉及的最复杂的操作是求平均，但是如果我们把其中的加法和乘法操作换成卷积，那么其实和神经网络类似。我们可以简单地画一下它的计算图：</p>
<p><img src="/%E6%89%BF/pytorch-samples/v2-1781041624f4c9fb31df04d11dd6a84a_1440w.jpg" alt="img"></p>
<p>下面给出了对应的代码，我们定义了<code>input</code>，<code>w1</code>，<code>w2</code>，<code>w3</code> 这三个变量，其中 <code>input</code> 不需要求导结果。根据 PyTorch 默认的求导规则，对于 <code>l1</code> 来说，因为有一个输入需要求导（也就是 <code>w1</code> 需要），所以它自己默认也需要求导，即 <code>requires_grad=True</code>。在整张计算图中，只有 <code>input</code> 一个变量是 <code>requires_grad=False</code> 的。正向传播过程的具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.ones([<span class="number">2</span>, <span class="number">2</span>], requires_grad=<span class="literal">False</span>)</span><br><span class="line">w1 = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w3 = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">l1 = <span class="built_in">input</span> * w1</span><br><span class="line">l2 = l1 + w2</span><br><span class="line">l3 = l1 * w3</span><br><span class="line">l4 = l2 * l3</span><br><span class="line">loss = l4.mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(w1.data, w1.grad, w1.grad_fn)</span><br><span class="line"><span class="comment"># tensor(2.) None None</span></span><br><span class="line"></span><br><span class="line">print(l1.data, l1.grad, l1.grad_fn)</span><br><span class="line"><span class="comment"># tensor([[2., 2.],</span></span><br><span class="line"><span class="comment">#         [2., 2.]]) None &lt;MulBackward0 object at 0x000001EBE79E6AC8&gt;</span></span><br><span class="line"></span><br><span class="line">print(loss.data, loss.grad, loss.grad_fn)</span><br><span class="line"><span class="comment"># tensor(40.) None &lt;MeanBackward0 object at 0x000001EBE79D8208&gt;</span></span><br></pre></td></tr></table></figure>

<p>正向传播的结果基本符合我们的预期。我们可以看到，变量 <code>l1</code> 的 <code>grad_fn</code> 储存着乘法操作符 <code>&lt;MulBackward0&gt;</code>，用于在反向传播中指导导数的计算。而 <code>w1</code> 是用户自己定义的，不是通过计算得来的，所以其 <code>grad_fn</code> 为空；同时因为还没有进行反向传播，<code>grad</code> 的值也为空。接下来，我们看一下如果要继续进行反向传播，计算图应该是什么样子：</p>
<p><img src="/%E6%89%BF/pytorch-samples/v2-18add4601e35e4b26fb73a50245e8de7_1440w.jpg" alt="img"></p>
<p>反向图也比较简单，从 <code>loss</code> 这个变量开始，通过链式法则，依次计算出各部分的导数。说到这里，我们不妨先自己手动推导一下求导的结果，再与程序运行结果作对比。如果对这部分不感兴趣的读者，可以直接跳过。</p>
<p>再摆一下公式：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">input = [1.0, 1.0, 1.0, 1.0]</span><br><span class="line">w1 = [2.0, 2.0, 2.0, 2.0]</span><br><span class="line">w2 = [3.0, 3.0, 3.0, 3.0]</span><br><span class="line">w3 = [4.0, 4.0, 4.0, 4.0]</span><br><span class="line"></span><br><span class="line">l1 = input x w1 = [2.0, 2.0, 2.0, 2.0]</span><br><span class="line">l2 = l1 + w2 = [5.0, 5.0, 5.0, 5.0]</span><br><span class="line">l3 = l1 x w3 = [8.0, 8.0, 8.0, 8.0] </span><br><span class="line">l4 = l2 x l3 = [40.0, 40.0, 40.0, 40.0] </span><br><span class="line">loss = mean(l4) = 40.0</span><br></pre></td></tr></table></figure>

<p><img src="/%E6%89%BF/pytorch-samples/image-20210420220025404.png" alt="image-20210420220025404"></p>
<p>其他的导数计算基本上都类似，因为过程太多，这里就不全写出来了，如果有兴趣的话大家不妨自己继续算一下。</p>
<p>接下来我们继续运行代码，并检查一下结果和自己算的是否一致：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(w1.grad, w2.grad, w3.grad)</span><br><span class="line"><span class="comment"># tensor(28.) tensor(8.) tensor(10.)</span></span><br><span class="line">print(l1.grad, l2.grad, l3.grad, l4.grad, loss.grad)</span><br><span class="line"><span class="comment"># None None None None None</span></span><br></pre></td></tr></table></figure>

<p>首先我们需要注意一下的是，在之前写程序的时候我们给定的 <code>w</code> 们都是一个常数，利用了广播的机制实现和常数和矩阵的加法乘法，比如 <code>w2 + l1</code>，实际上我们的程序会自动把 <code>w2</code> 扩展成 [[3.0, 3.0], [3.0, 3.0]]，和 <code>l1</code> 的形状一样之后，再进行加法计算，计算的导数结果实际上为 [[2.0, 2.0], [2.0, 2.0]]，为了对应常数输入，所以最后 <code>w2</code> 的梯度返回为矩阵之和 8 。另外还有一个问题，虽然 <code>w</code> 开头的那些和我们的计算结果相符，但是为什么 <code>l1</code>，<code>l2</code>，<code>l3</code>，甚至其他的部分的求导结果都为空呢？想要解答这个问题，我们得明白什么是叶子张量。</p>
<h3 id="叶子张量"><a href="#叶子张量" class="headerlink" title="叶子张量"></a>叶子张量</h3><p>对于任意一个张量来说，我们可以用 <code>tensor.is_leaf</code> 来判断它是否是叶子张量（leaf tensor）。在反向传播过程中，只有 <code>is_leaf=True</code> 的时候，需要求导的张量的导数结果才会被最后保留下来。</p>
<p>对于 <code>requires_grad=False</code> 的 tensor 来说，我们约定俗成地把它们归为叶子张量。但其实无论如何划分都没有影响，因为张量的 <code>is_leaf</code> 属性只有在需要求导的时候才有意义。</p>
<p>我们真正需要注意的是当 <code>requires_grad=True</code> 的时候，如何判断是否是叶子张量：当这个 tensor 是用户创建的时候，它是一个叶子节点，当这个 tensor 是由其他运算操作产生的时候，它就不是一个叶子节点。我们来看个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones([<span class="number">2</span>, <span class="number">2</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(a.is_leaf)</span><br><span class="line"><span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">b = a + <span class="number">2</span></span><br><span class="line">print(b.is_leaf)</span><br><span class="line"><span class="comment"># False</span></span><br><span class="line"><span class="comment"># 因为 b 不是用户创建的，是通过计算生成的</span></span><br></pre></td></tr></table></figure>

<p>这时有同学可能会问了，为什么要搞出这么个叶子张量的概念出来？原因是为了节省内存（或显存）。我们来想一下，那些非叶子结点，是通过用户所定义的叶子节点的一系列运算生成的，也就是这些非叶子节点都是中间变量，一般情况下，用户不会去使用这些中间变量的导数，所以为了节省内存，它们在用完之后就被释放了。</p>
<p>我们回头看一下之前的反向传播计算图，在图中的叶子节点我用绿色标出了。可以看出来，被叫做叶子，可能是因为游离在主干之外，没有子节点，因为它们都是被用户创建的，不是通过其他节点生成。对于叶子节点来说，它们的 <code>grad_fn</code> 属性都为空；而对于非叶子结点来说，因为它们是通过一些操作生成的，所以它们的 <code>grad_fn</code> 不为空。</p>
<p>我们有办法保留中间变量的导数吗？当然有，通过使用 <code>tensor.retain_grad()</code> 就可以：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 和前边一样</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">loss = l4.mean()</span><br><span class="line"></span><br><span class="line">l1.retain_grad()</span><br><span class="line">l4.retain_grad()</span><br><span class="line">loss.retain_grad()</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(loss.grad)</span><br><span class="line"><span class="comment"># tensor(1.)</span></span><br><span class="line">print(l4.grad)</span><br><span class="line"><span class="comment"># tensor([[0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500]])</span></span><br><span class="line">print(l1.grad)</span><br><span class="line"><span class="comment"># tensor([[7., 7.],</span></span><br><span class="line"><span class="comment">#         [7., 7.]])</span></span><br></pre></td></tr></table></figure>

<p>如果我们只是想进行 debug，只需要输出中间变量的导数信息，而不需要保存它们，我们还可以使用 <code>tensor.register_hook</code>，例子如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 和前边一样</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">loss = l4.mean()</span><br><span class="line"></span><br><span class="line">l1.register_hook(<span class="keyword">lambda</span> grad: print(<span class="string">&#x27;l1 grad: &#x27;</span>, grad))</span><br><span class="line">l4.register_hook(<span class="keyword">lambda</span> grad: print(<span class="string">&#x27;l4 grad: &#x27;</span>, grad))</span><br><span class="line">loss.register_hook(<span class="keyword">lambda</span> grad: print(<span class="string">&#x27;loss grad: &#x27;</span>, grad))</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss grad:  tensor(1.)</span></span><br><span class="line"><span class="comment"># l4 grad:  tensor([[0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500]])</span></span><br><span class="line"><span class="comment"># l1 grad:  tensor([[7., 7.],</span></span><br><span class="line"><span class="comment">#         [7., 7.]])</span></span><br><span class="line"></span><br><span class="line">print(loss.grad)</span><br><span class="line"><span class="comment"># None</span></span><br><span class="line"><span class="comment"># loss 的 grad 在 print 完之后就被清除掉了</span></span><br></pre></td></tr></table></figure>

<p>这个函数的功能远远不止打印导数信息用以 debug，但是一般很少用，所以这里就不扩展了，更多请参考知乎提问 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/61044004">pytorch中的钩子（Hook）有何作用？</a></p>
<p>到此为止，我们已经讨论完了这个实例中的正向传播和反向传播的有关内容了。回过头来看， input 其实很像神经网络输入的图像，w1, w2, w3 则类似卷积核的参数，而 l1, l2, l3, l4 可以表示4个卷积层输出，如果我们把节点上的加法乘法换成卷积操作的话。实际上这个简单的模型，很像我们平时的神经网络的简化版，通过这个例子，相信大家多少也能对神经网络的正向和反向传播过程有个大致的了解了吧。</p>
<h3 id="inplace-操作"><a href="#inplace-操作" class="headerlink" title="inplace 操作"></a>inplace 操作</h3><p>现在我们来看一下本篇的重点，inplace operation。可以说，我们求导时候大部分的 bug，都出在使用了 inplace 操作上。现在我们以 PyTorch 不同的报错信息作为驱动，来讲一讲 inplace 操作吧。第一个报错信息：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: balabala...</span><br></pre></td></tr></table></figure>

<p>不少人可能会感到很熟悉，没错，我就是其中之一。之前写代码的时候竟经常报这个错，原因是对 inplace 操作不了解。要搞清楚为什么会报错，我们先来了解一下什么是 inplace 操作：inplace 指的是在不更改变量的内存地址的情况下，直接修改变量的值。我们来看两种情况，大家觉得这两种情况哪个是 inplace 操作，哪个不是？或者两个都是 inplace？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 情景 1</span></span><br><span class="line">a = a.exp()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情景 2</span></span><br><span class="line">a[<span class="number">0</span>] = <span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>答案是：情景1不是 inplace，类似 Python 中的 <code>i=i+1</code>, 而情景2是 inplace 操作，类似 <code>i+=1</code>。依稀记得当时做机器学习的大作业，很多人都被其中一个 <code>i+=1</code> 和 <code>i=i+1</code> 问题给坑了好长时间。那我们来实际测试一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们要用到 id() 这个函数，其返回值是对象的内存地址</span></span><br><span class="line"><span class="comment"># 情景 1</span></span><br><span class="line">a = torch.tensor([<span class="number">3.0</span>, <span class="number">1.0</span>])</span><br><span class="line">print(<span class="built_in">id</span>(a)) <span class="comment"># 2112716404344</span></span><br><span class="line">a = a.exp()</span><br><span class="line">print(<span class="built_in">id</span>(a)) <span class="comment"># 2112715008904</span></span><br><span class="line"><span class="comment"># 在这个过程中 a.exp() 生成了一个新的对象，然后再让 a</span></span><br><span class="line"><span class="comment"># 指向它的地址，所以这不是个 inplace 操作</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 情景 2</span></span><br><span class="line">a = torch.tensor([<span class="number">3.0</span>, <span class="number">1.0</span>])</span><br><span class="line">print(<span class="built_in">id</span>(a)) <span class="comment"># 2112716403840</span></span><br><span class="line">a[<span class="number">0</span>] = <span class="number">10</span></span><br><span class="line">print(<span class="built_in">id</span>(a), a) <span class="comment"># 2112716403840 tensor([10.,  1.])</span></span><br><span class="line"><span class="comment"># inplace 操作，内存地址没变</span></span><br></pre></td></tr></table></figure>

<p>PyTorch 是怎么检测 tensor 发生了 inplace 操作呢？答案是通过 <code>tensor._version</code> 来检测的。我们还是来看个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1.0</span>, <span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a + <span class="number">2</span></span><br><span class="line">print(b._version) <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line">loss = (b * b).mean()</span><br><span class="line">b[<span class="number">0</span>] = <span class="number">1000.0</span></span><br><span class="line">print(b._version) <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation ...</span></span><br></pre></td></tr></table></figure>

<p>每次 tensor 在进行 inplace 操作时，变量 <code>_version</code> 就会加1，其初始值为0。在正向传播过程中，求导系统记录的 <code>b</code> 的 version 是0，但是在进行反向传播的过程中，求导系统发现 <code>b</code> 的 version 变成1了，所以就会报错了。但是还有一种特殊情况不会报错，就是反向传播求导的时候如果没用到 <code>b</code> 的值（比如 <code>y=x+1</code>， y 关于 x 的导数是1，和 x 无关），自然就不会去对比 <code>b</code> 前后的 version 了，所以不会报错。</p>
<p>上边我们所说的情况是针对非叶子节点的，对于 <code>requires_grad=True</code> 的叶子节点来说，要求更加严格了，甚至在叶子节点被使用之前修改它的值都不行。我们来看一个报错信息：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: leaf variable has been moved into the graph interior</span><br></pre></td></tr></table></figure>

<p>这个意思通俗一点说就是你的一顿 inplace 操作把一个叶子节点变成了非叶子节点了。我们知道，非叶子节点的导数在默认情况下是不会被保存的，这样就会出问题了。举个小例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">10.</span>, <span class="number">5.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(a, a.is_leaf)</span><br><span class="line"><span class="comment"># tensor([10.,  5.,  2.,  3.], requires_grad=True) True</span></span><br><span class="line"></span><br><span class="line">a[:] = <span class="number">0</span></span><br><span class="line">print(a, a.is_leaf)</span><br><span class="line"><span class="comment"># tensor([0., 0., 0., 0.], grad_fn=&lt;CopySlices&gt;) False</span></span><br><span class="line"></span><br><span class="line">loss = (a*a).mean()</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># RuntimeError: leaf variable has been moved into the graph interior</span></span><br></pre></td></tr></table></figure>

<p>我们看到，在进行对 <code>a</code> 的重新 inplace 赋值之后，表示了 a 是通过 copy operation 生成的，<code>grad_fn</code> 都有了，所以自然而然不是叶子节点了。本来是该有导数值保留的变量，现在成了导数会被自动释放的中间变量了，所以 PyTorch 就给你报错了。还有另外一种情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">10.</span>, <span class="number">5.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">a.add_(<span class="number">10.</span>) <span class="comment"># 或者 a += 10.</span></span><br><span class="line"><span class="comment"># RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.</span></span><br></pre></td></tr></table></figure>

<p>这个更厉害了，不等到你调用 backward，只要你对需要求导的叶子张量使用了这些操作，马上就会报错。那是不是需要求导的叶子节点一旦被初始化赋值之后，就不能修改它们的值了呢？我们如果在某种情况下需要重新对叶子变量赋值该怎么办呢？有办法！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一</span></span><br><span class="line">a = torch.tensor([<span class="number">10.</span>, <span class="number">5.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(a, a.is_leaf, <span class="built_in">id</span>(a))</span><br><span class="line"><span class="comment"># tensor([10.,  5.,  2.,  3.], requires_grad=True) True 2501274822696</span></span><br><span class="line"></span><br><span class="line">a.data.fill_(<span class="number">10.</span>)</span><br><span class="line"><span class="comment"># 或者 a.detach().fill_(10.)</span></span><br><span class="line">print(a, a.is_leaf, <span class="built_in">id</span>(a))</span><br><span class="line"><span class="comment"># tensor([10., 10., 10., 10.], requires_grad=True) True 2501274822696</span></span><br><span class="line"></span><br><span class="line">loss = (a*a).mean()</span><br><span class="line">loss.backward()</span><br><span class="line">print(a.grad)</span><br><span class="line"><span class="comment"># tensor([5., 5., 5., 5.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二</span></span><br><span class="line">a = torch.tensor([<span class="number">10.</span>, <span class="number">5.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(a, a.is_leaf)</span><br><span class="line"><span class="comment"># tensor([10.,  5.,  2.,  3.], requires_grad=True) True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    a[:] = <span class="number">10.</span></span><br><span class="line">print(a, a.is_leaf)</span><br><span class="line"><span class="comment"># tensor([10., 10., 10., 10.], requires_grad=True) True</span></span><br><span class="line"></span><br><span class="line">loss = (a*a).mean()</span><br><span class="line">loss.backward()</span><br><span class="line">print(a.grad)</span><br><span class="line"><span class="comment"># tensor([5., 5., 5., 5.])</span></span><br></pre></td></tr></table></figure>

<p>修改的方法有很多种，核心就是修改那个和变量共享内存，但 <code>requires_grad=False</code> 的版本的值，比如通过 <code>tensor.data</code> 或者 <code>tensor.detach()</code>。我们需要注意的是，要在变量被使用之前修改，不然等计算完之后再修改，还会造成求导上的问题，会报错的。</p>
<hr>
<p>为什么 PyTorch 的求导不支持绝大部分 inplace 操作呢？从上边我们也看出来了，因为真的很 tricky。比如有的时候在一个变量已经参与了正向传播的计算，之后它的值被修改了，在做反向传播的时候如果还需要这个变量的值的话，我们肯定不能用那个后来修改的值吧，但没修改之前的原始值已经被释放掉了，我们怎么办？一种可行的办法就是我们在 Function 做 forward 的时候每次都开辟一片空间储存当时输入变量的值，这样无论之后它们怎么修改，都不会影响了，反正我们有备份在存着。但这样有什么问题？这样会导致内存（或显存）使用量大大增加。因为我们不确定哪个变量可能之后会做 inplace 操作，所以我们每个变量在做完 forward 之后都要储存一个备份，成本太高了。除此之外，inplace operation 还可能造成很多其他求导上的问题。</p>
<p>总之，我们在实际写代码的过程中，没有必须要用 inplace operation 的情况，而且支持它会带来很大的性能上的牺牲，所以 PyTorch 不推荐使用 inplace 操作，当求导过程中发现有 inplace 操作影响求导正确性的时候，会采用报错的方式提醒。但这句话反过来说就是，因为只要有 inplace 操作不当就会报错，所以如果我们在程序中使用了 inplace 操作却没报错，那么说明我们最后求导的结果是正确的，没问题的。这就是我们常听见的<strong>没报错就没有问题</strong>。</p>
<h3 id="动态图，静态图"><a href="#动态图，静态图" class="headerlink" title="动态图，静态图"></a>动态图，静态图</h3><p>可能大家都听说过，PyTorch 使用的是动态图（Dynamic Computational Graphs）的方式，而 TensorFlow 使用的是静态图（Static Computational Graphs）。所以二者究竟有什么区别呢，我们本节来就来讨论这个事情。</p>
<p>所谓动态图，就是每次当我们搭建完一个计算图，然后在反向传播结束之后，整个计算图就在内存中被释放了。如果想再次使用的话，必须从头再搭一遍，参见下边这个例子。而以 TensorFlow 为代表的静态图，每次都先设计好计算图，需要的时候实例化这个图，然后送入各种输入，重复使用，只有当会话结束的时候创建的图才会被释放（不知道这里我对 tf.Session 的理解对不对，如果有错误希望大佬们能指正一下），就像我们之前举的那个水管的例子一样，设计好水管布局之后，需要用的时候就开始搭，搭好了就往入口加水，什么时候不需要了，再把管道都给拆了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是一个关于 PyTorch 是动态图的例子：</span></span><br><span class="line">a = torch.tensor([<span class="number">3.0</span>, <span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a * a</span><br><span class="line">loss = b.mean()</span><br><span class="line"></span><br><span class="line">loss.backward() <span class="comment"># 正常</span></span><br><span class="line">loss.backward() <span class="comment"># RuntimeError</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二次：从头再来一遍</span></span><br><span class="line">a = torch.tensor([<span class="number">3.0</span>, <span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a * a</span><br><span class="line">loss = b.mean()</span><br><span class="line">loss.backward() <span class="comment"># 正常</span></span><br></pre></td></tr></table></figure>

<p>从描述中我们可以看到，理论上来说，静态图在效率上比动态图要高。因为首先，静态图只用构建一次，然后之后重复使用就可以了；其次静态图因为是固定不需要改变的，所以在设计完了计算图之后，可以进一步的优化，比如可以将用户原本定义的 Conv 层和 ReLU 层合并成 ConvReLU 层，提高效率。</p>
<p>但是，深度学习框架的速度不仅仅取决于图的类型，还很其他很多因素，比如底层代码质量，所使用的底层 BLAS 库等等等都有关。从实际测试结果来说，至少在主流的模型的训练时间上，PyTorch 有着至少不逊于静态图框架 Caffe，TensorFlow 的表现。具体对比数据可以参考 <a href="https://link.zhihu.com/?target=https://github.com/ilkarman/DeepLearningFrameworks">这个 GitHub 仓库</a>。</p>
<p>大家不要急着纠正我，我知道，我现在就说：当然，在 9102 年的今天，动态图和静态图直接的界限已经开始慢慢模糊。PyTorch 模型转成 Caffe 模型越来越方便，而 TensorFlow 也加入了一些动态图机制。</p>
<p>除了动态图之外，PyTorch 还有一个特性，叫 eager execution。意思就是当遇到 tensor 计算的时候，马上就回去执行计算，也就是，实际上 PyTorch 根本不会去构建正向计算图，而是遇到操作就执行。真正意义上的正向计算图是把所有的操作都添加完，构建好了之后，再运行神经网络的正向传播。</p>
<p>正是因为 PyTorch 的两大特性：动态图和 eager execution，所以它用起来才这么顺手，简直就和写 Python 程序一样舒服，debug 也非常方便。除此之外，我们从之前的描述也可以看出，PyTorch 十分注重占用内存（或显存）大小，没有用的空间释放很及时，可以很有效地利用有限的内存。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章主要讨论了 PyTorch 的 Autograd 机制和使用 inplace 操作不当可能会导致的各种报错。在实际写代码的过程中，涉及需要求导的部分，不建议大家使用 inplace 操作。除此之外我们还比较了动态图和静态图框架，PyTorch 作为动态图框架的代表之一，对初学者非常友好，而且运行速度上不逊于静态图框架，再加上现在通过 ONNX 转换为其他框架的模型用以部署也越来越方便，我觉得是一个非常称手的深度学习工具。</p>
<h2 id="参考资料-1"><a href="#参考资料-1" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://link.zhihu.com/?target=https://pytorch.org/docs/stable/notes/autograd.html">PyTorch Docs: AUTOGRAD MECHANICS</a></li>
<li><a href="https://link.zhihu.com/?target=https://www.youtube.com/watch?v=MswxJw-8PvE">YouTube 英文视频：PyTorch Autograd Explained - In-depth Tutorial</a></li>
<li><a href="https://link.zhihu.com/?target=http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html">Inplace operation in pytorch</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38475183">关于 pytorch inplace operation, 需要知道的几件事</a></li>
<li><a href="https://link.zhihu.com/?target=http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture06.pdf">cs231n 2019 lecture 6: Hardware and Software</a></li>
<li><a href="https://link.zhihu.com/?target=https://openreview.net/forum?id=BJJsrmfCZ">Automatic differentiation in PyTorch</a></li>
<li><a href="https://link.zhihu.com/?target=https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec">Understanding how Automatic Differentiation works</a></li>
</ol>
<h1 id="PyTorch：定义新的-Autograd-函数"><a href="#PyTorch：定义新的-Autograd-函数" class="headerlink" title="PyTorch：定义新的 Autograd 函数"></a>PyTorch：定义新的 Autograd 函数</h1><p>经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测<code>y = sin(x)</code>从<code>-pi</code>到<code>pi</code>。 而不是将多项式写为<code>y = a + bx + cx ^ 2 + dx ^ 3</code>，我们将多项式写为<code>y = a + b P[3](c + dx)</code>其中<code>P[3](x) = 1/2 (5x ^ 3 - 3x)</code>是三次的<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Legendre_polynomials">勒让德多项式</a>。</p>
<p>此实现使用 PyTorch 张量上的运算来计算正向传播，并使用 PyTorch Autograd 来计算梯度。</p>
<p>在此实现中，我们实现了自己的自定义 Autograd 函数来执行<code>P&#39;[3](x)</code>。 通过数学，<code>P&#39;[3](x) = 3/2 (5x ^ 2 - 1)</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LegendrePolynomial3</span>(<span class="params">torch.autograd.Function</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    我们可以通过子类化实现我们自己的自定义autograd函数torch.autograd.功能实现对张量进行操作的向前和向后传球。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        在向前传递中，我们接收包含输入的张量，并返回包含输出的张量。</span></span><br><span class="line"><span class="string">        ctx是一个上下文对象，可以用来为向后计算存储信息。可以使用ctx.save_for_backward方法。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * (<span class="number">5</span> * <span class="built_in">input</span> ** <span class="number">3</span> - <span class="number">3</span> * <span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, grad_output</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        在后向过程中，我们接收到一个张量，其中包含了与输出有关的损耗梯度，并且我们需要计算与输入有关的损耗梯度。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        <span class="keyword">return</span> grad_output * <span class="number">1.5</span> * (<span class="number">5</span> * <span class="built_in">input</span> ** <span class="number">2</span> - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">dtype = torch.<span class="built_in">float</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># device = torch.device(&quot;cuda:0&quot;)  # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Tensors to hold input and outputs.</span></span><br><span class="line"><span class="comment"># By default, requires_grad=False, which indicates that we do not need to</span></span><br><span class="line"><span class="comment"># compute gradients with respect to these Tensors during the backward pass.</span></span><br><span class="line">x = torch.linspace(-math.pi, math.pi, <span class="number">2000</span>, device=device, dtype=dtype)</span><br><span class="line">y = torch.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights. For this example, we need</span></span><br><span class="line"><span class="comment"># 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized</span></span><br><span class="line"><span class="comment"># not too far from the correct result to ensure convergence.</span></span><br><span class="line"><span class="comment"># 为权重创建随机张量。对于这个例子，我们需要4个权值：y=a+b*P3（c+d*x），这些权值需要在离正确结果不太远的地方初始化以确保收敛。</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Tensors during the backward pass.</span></span><br><span class="line"><span class="comment"># 设置requires_grad=True表示我们希望在向后传递期间计算相对于这些张量的梯度。</span></span><br><span class="line">a = torch.full((), <span class="number">0.0</span>, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.full((), -<span class="number">1.0</span>, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = torch.full((), <span class="number">0.0</span>, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = torch.full((), <span class="number">0.3</span>, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">5e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):</span><br><span class="line">    <span class="comment"># To apply our Function, we use Function.apply method. We alias this as &#x27;P3&#x27;.</span></span><br><span class="line">    P3 = LegendrePolynomial3.apply</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations; we compute</span></span><br><span class="line">    <span class="comment"># P3 using our custom autograd operation.</span></span><br><span class="line">    y_pred = a + b * P3(c + d * x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        print(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        a -= learning_rate * a.grad</span><br><span class="line">        b -= learning_rate * b.grad</span><br><span class="line">        c -= learning_rate * c.grad</span><br><span class="line">        d -= learning_rate * d.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        a.grad = <span class="literal">None</span></span><br><span class="line">        b.grad = <span class="literal">None</span></span><br><span class="line">        c.grad = <span class="literal">None</span></span><br><span class="line">        d.grad = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;Result: y = <span class="subst">&#123;a.item()&#125;</span> + <span class="subst">&#123;b.item()&#125;</span> * P3(<span class="subst">&#123;c.item()&#125;</span> + <span class="subst">&#123;d.item()&#125;</span> x)&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/%E6%89%BF/pytorch-samples/autograd.png" alt="autograd"></p>
<h2 id="staticmethod和-classmethod的用法"><a href="#staticmethod和-classmethod的用法" class="headerlink" title="@staticmethod和@classmethod的用法"></a>@staticmethod和@classmethod的用法</h2><h3 id="讲解一"><a href="#讲解一" class="headerlink" title="讲解一"></a>讲解一</h3><p><strong>一般来说，要使用某个类的方法，需要先实例化一个对象再调用方法。</strong><br><strong>而使用@staticmethod或@classmethod，就可以不需要实例化，直接类名.方法名()来调用。</strong><br>这有利于组织代码，把某些应该属于某个类的函数给放到那个类里去，同时有利于命名空间的整洁。</p>
<p>既然@staticmethod和@classmethod都可以直接类名.方法名()来调用，那他们有什么区别呢<br>从它们的使用上来看,</p>
<ul>
<li><p>@staticmethod不需要表示自身对象的self和自身类的cls参数，就跟使用函数一样。</p>
</li>
<li><p>@classmethod也不需要self参数，但第一个参数需要是表示自身类的cls参数。</p>
<p>如果在@staticmethod中要调用到这个类的一些属性方法，只能直接类名.属性名或类名.方法名。<br>而@classmethod因为持有cls参数，可以来调用类的属性，类的方法，实例化对象等，避免硬编码。<br>下面上代码。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>(<span class="params"><span class="built_in">object</span></span>):</span>  </span><br><span class="line">    bar = <span class="number">1</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">self</span>):</span>  </span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;foo&#x27;</span>  </span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">static_foo</span>():</span>  </span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;static_foo&#x27;</span>  </span><br><span class="line">        <span class="built_in">print</span> A.bar  </span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">class_foo</span>(<span class="params">cls</span>):</span>  </span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;class_foo&#x27;</span>  </span><br><span class="line">        <span class="built_in">print</span> cls.bar  </span><br><span class="line">        cls().foo()  </span><br><span class="line"><span class="comment">###执行  </span></span><br><span class="line">A.static_foo()  </span><br><span class="line">A.class_foo()  </span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">static_foo</span><br><span class="line"><span class="number">1</span></span><br><span class="line">class_foo</span><br><span class="line"><span class="number">1</span></span><br><span class="line">foo</span><br></pre></td></tr></table></figure>

<h3 id="讲解二"><a href="#讲解二" class="headerlink" title="讲解二"></a>讲解二</h3><p>类中最常用的方法是实例方法, 即通过通过实例作为第一个参数的方法。<br>举个例子，一个基本的实例方法就向下面这个:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kls</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        self.data = data</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printd</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(self.data)</span><br><span class="line">ik1 = Kls(<span class="string">&#x27;arun&#x27;</span>)</span><br><span class="line">ik2 = Kls(<span class="string">&#x27;seema&#x27;</span>)</span><br><span class="line">ik1.printd()</span><br><span class="line">ik2.printd()</span><br></pre></td></tr></table></figure>

<p>这会给出如下的输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arun</span><br><span class="line">seema</span><br></pre></td></tr></table></figure>

<p><img src="/%E6%89%BF/pytorch-samples/20160922171529212" alt="这里写图片描述"><br>然后看一下代码和示例图片:</p>
<ul>
<li>1，2参数传递给方法.</li>
<li>3 self参数指向当前实例自身.</li>
<li>4 我们不需要传递实例自身给方法，Python解释器自己会做这些操作的</li>
</ul>
<p><strong>如果现在我们想写一些仅仅与类交互而不是和实例交互的方法</strong>会怎么样呢? 我们可以在类外面写一个简单的方法来做这些，但是这样做就扩散了类代码的关系到类定义的外面. 如果像下面这样写就会导致以后代码维护的困难:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_no_of_instances</span>(<span class="params">cls_obj</span>):</span></span><br><span class="line">    <span class="keyword">return</span> cls_obj.no_inst</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kls</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    no_inst = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Kls.no_inst = Kls.no_inst + <span class="number">1</span></span><br><span class="line">ik1 = Kls()</span><br><span class="line">ik2 = Kls()</span><br><span class="line">print(get_no_of_instances(Kls))</span><br></pre></td></tr></table></figure>

<p>输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>@classmethod<br>我们要写一个只在类中运行而不在实例中运行的方法. 如果我们想让方法不在实例中运行，可以这么做:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iget_no_of_instance</span>(<span class="params">ins_obj</span>):</span></span><br><span class="line">    <span class="keyword">return</span> ins_obj.__class__.no_inst</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kls</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    no_inst = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    Kls.no_inst = Kls.no_inst + <span class="number">1</span></span><br><span class="line">ik1 = Kls()</span><br><span class="line">ik2 = Kls()</span><br><span class="line"><span class="built_in">print</span> iget_no_of_instance(ik1)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>在Python2.2以后可以使用@classmethod装饰器来创建类方法.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kls</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    no_inst = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Kls.no_inst = Kls.no_inst + <span class="number">1</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_no_of_instance</span>(<span class="params">cls_obj</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls_obj.no_inst</span><br><span class="line">ik1 = Kls()</span><br><span class="line">ik2 = Kls()</span><br><span class="line"><span class="built_in">print</span> ik1.get_no_of_instance()</span><br><span class="line"><span class="built_in">print</span> Kls.get_no_of_instance()</span><br></pre></td></tr></table></figure>

<p>输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>这样的好处是: 不管这个方式是从实例调用还是从类调用，它都用第一个参数把类传递过来.<br>@staticmethod<br>经常有一些跟类有关系的功能但在运行时又不需要实例和类参与的情况下需要用到静态方法. 比如更改环境变量或者修改其他类的属性等能用到静态方法. 这种情况可以直接用函数解决, 但这样同样会扩散类内部的代码，造成维护困难.<br>比如这样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">IND = <span class="string">&#x27;ON&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkind</span>():</span></span><br><span class="line">    <span class="keyword">return</span> (IND == <span class="string">&#x27;ON&#x27;</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kls</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,data</span>):</span></span><br><span class="line">        self.data = data</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">if</span> checkind():</span><br><span class="line">        print(<span class="string">&#x27;Reset done for:&#x27;</span>, self.data)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_db</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">if</span> checkind():</span><br><span class="line">        self.db = <span class="string">&#x27;new db connection&#x27;</span></span><br><span class="line">        print(<span class="string">&#x27;DB connection made for:&#x27;</span>,self.data)</span><br><span class="line">ik1 = Kls(<span class="number">12</span>)</span><br><span class="line">ik1.do_reset()</span><br><span class="line">ik1.set_db()</span><br></pre></td></tr></table></figure>

<p>输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Reset done <span class="keyword">for</span>: <span class="number">12</span></span><br><span class="line">DB connection made <span class="keyword">for</span>: <span class="number">12</span></span><br></pre></td></tr></table></figure>

<p>如果使用@staticmethod就能把相关的代码放到对应的位置了.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">IND = <span class="string">&#x27;ON&#x27;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kls</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        self.data = data</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">checkind</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (IND == <span class="string">&#x27;ON&#x27;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.checkind():</span><br><span class="line">            print(<span class="string">&#x27;Reset done for:&#x27;</span>, self.data)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_db</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.checkind():</span><br><span class="line">            self.db = <span class="string">&#x27;New db connection&#x27;</span></span><br><span class="line">        print(<span class="string">&#x27;DB connection made for: &#x27;</span>, self.data)</span><br><span class="line">ik1 = Kls(<span class="number">12</span>)</span><br><span class="line">ik1.do_reset()</span><br><span class="line">ik1.set_db()</span><br></pre></td></tr></table></figure>

<p>输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Reset done <span class="keyword">for</span>: <span class="number">12</span></span><br><span class="line">DB connection made <span class="keyword">for</span>: <span class="number">12</span></span><br></pre></td></tr></table></figure>

<p>下面这个更全面的代码和图示来展示这两种方法的不同<br>@staticmethod 和 @classmethod的不同</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kls</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        self.data = data</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printd</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(self.data)</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">smethod</span>(<span class="params">*arg</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;Static:&#x27;</span>, arg)</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cmethod</span>(<span class="params">*arg</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;Class:&#x27;</span>, arg)</span><br><span class="line">   <span class="number">12345678910111234567891011</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ik = Kls(<span class="number">23</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ik.printd()</span><br><span class="line"><span class="number">23</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ik.smethod()</span><br><span class="line">Static: ()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ik.cmethod()</span><br><span class="line">Class: (&lt;class &#x27;__main__.Kls&#x27;&gt;,)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Kls.printd()</span><br><span class="line">TypeError: unbound method printd() must be called <span class="keyword">with</span> Kls instance <span class="keyword">as</span> first argument (got nothing instead)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Kls.smethod()</span><br><span class="line">Static: ()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Kls.cmethod()</span><br><span class="line">Class: (&lt;class &#x27;__main__.Kls&#x27;&gt;,)</span><br></pre></td></tr></table></figure>

<p><img src="/%E6%89%BF/pytorch-samples/20160922172202855" alt="这里写图片描述"></p>
<h2 id="with-torch-no-grad-详解"><a href="#with-torch-no-grad-详解" class="headerlink" title="with torch.no_grad() 详解"></a>with torch.no_grad() 详解</h2><p>torch.no_grad() 是一个上下文管理器，被该语句 wrap 起来的部分将不会track 梯度。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1.1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a * <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>打印b可看到其 grad_fn 为 mulbackward 表示是做的乘法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b</span><br><span class="line">Out[<span class="number">63</span>]: tensor([<span class="number">2.2000</span>], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line"></span><br><span class="line">b.add_(<span class="number">2</span>)</span><br><span class="line">Out[<span class="number">64</span>]: tensor([<span class="number">4.2000</span>], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到不被wrap的情况下，b.grad_fn 为 addbackward 表示这个add 操作被track了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    b.mul_(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>在被包裹的情况下可以看到 b.grad_fn 还是为 add，mul 操作没有被 track. 但是注意，乘法操作是被执行了的。(4.2 -&gt; 8.4)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b</span><br><span class="line">Out[<span class="number">66</span>]: tensor([<span class="number">8.4000</span>], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>所以如果有不想被track的计算部分可以通过这么一个上下文管理器包裹起来。这样可以执行计算，但该计算不会在反向传播中被记录。</p>
<p>同时 torch.no_grad() 还可以作为一个装饰器。<br>比如在网络测试的函数前加上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>():</span></span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<p>扩展：<br>同样还可以用 torch.set_grad_enabled()来实现不计算梯度。<br>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>():</span></span><br><span class="line">	torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line">	...	<span class="comment"># your test code</span></span><br><span class="line">	torch.set_grad_enabled(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h1 id="PyTorch：nn"><a href="#PyTorch：nn" class="headerlink" title="PyTorch：nn"></a>PyTorch：<code>nn</code></h1><p>经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测<code>y = sin(x)</code>从<code>-pi</code>到<code>pi</code>。</p>
<p>此实现使用来自 PyTorch 的<code>nn</code>包来构建网络。 PyTorch Autograd 使定义计算图和获取梯度变得容易，但是原始的 Autograd 对于定义复杂的神经网络来说可能太低了。 这是<code>nn</code>包可以提供帮助的地方。 <code>nn</code>包定义了一组模块，您可以将其视为神经网络层，该神经网络层从输入产生输出并且可能具有一些可训练的权重。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      <div style="display: inline-block;">
        <img src="/images/pay.gif" alt="Bo ‎">
        <p>‎</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Pytorch/" rel="tag"># Pytorch</a>
          </div>

        


        
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%83%AD%E8%BA%AB%EF%BC%9ANumPy"><span class="nav-number">1.</span> <span class="nav-text">热身：NumPy</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-number">1.1.</span> <span class="nav-text">学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%AD%A6%E4%B9%A0%E7%8E%87-learning-rate"><span class="nav-number">1.1.1.</span> <span class="nav-text">深度学习: 学习率 (learning rate)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%A4%A7%E5%B0%8F"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">学习率大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%AE%BE%E7%BD%AE"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">学习率设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%87%8F%E7%BC%93%E6%9C%BA%E5%88%B6"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">学习率减缓机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%8A%E8%84%89-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%80%BC-%E6%9B%B2%E7%BA%BF"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">把脉 目标函数损失值 曲线</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E8%B7%B5"><span class="nav-number">1.1.2.</span> <span class="nav-text">实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1%E6%A2%AF%E5%BA%A6"><span class="nav-number">1.2.</span> <span class="nav-text">计算损失梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.3.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch%EF%BC%9A%E5%BC%A0%E9%87%8F"><span class="nav-number">2.</span> <span class="nav-text">PyTorch：张量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">张量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">2.1.1.</span> <span class="nav-text">张量是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7%E5%92%8C%E6%80%A7%E8%B4%A8"><span class="nav-number">2.1.2.</span> <span class="nav-text">张量的属性和性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-attributes"><span class="nav-number">2.1.3.</span> <span class="nav-text">Tensor attributes:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAtensor"><span class="nav-number">2.1.4.</span> <span class="nav-text">创建tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%93%8D%E4%BD%9Ctensor"><span class="nav-number">2.1.5.</span> <span class="nav-text">操作tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor%E6%93%8D%E4%BD%9C"><span class="nav-number">2.1.6.</span> <span class="nav-text">Tensor操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tips"><span class="nav-number">2.1.7.</span> <span class="nav-text">Tips:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="nav-number">2.1.8.</span> <span class="nav-text">张量的创建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%9B%B4%E6%8E%A5%E5%88%9B%E5%BB%BA"><span class="nav-number">2.1.8.1.</span> <span class="nav-text">1. 直接创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BE%9D%E6%8D%AE%E6%95%B0%E5%80%BC%E5%88%9B%E5%BB%BA"><span class="nav-number">2.1.8.2.</span> <span class="nav-text">2. 依据数值创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BE%9D%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="nav-number">2.1.8.3.</span> <span class="nav-text">3. 依概率分布创建张量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%9A%E6%8B%BC%E6%8E%A5%E3%80%81%E5%88%87%E5%88%86%E3%80%81%E7%B4%A2%E5%BC%95%E5%92%8C%E5%8F%98%E6%8D%A2"><span class="nav-number">2.1.9.</span> <span class="nav-text">张量的操作：拼接、切分、索引和变换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E6%8B%BC%E6%8E%A5%E4%B8%8E%E5%88%87%E5%88%86"><span class="nav-number">2.1.9.1.</span> <span class="nav-text">张量拼接与切分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%8F%98%E6%8D%A2"><span class="nav-number">2.1.9.2.</span> <span class="nav-text">张量索引与变换</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="nav-number">2.1.10.</span> <span class="nav-text">张量数学运算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch%EF%BC%9A%E5%BC%A0%E9%87%8F%E5%92%8C-Autograd"><span class="nav-number">3.</span> <span class="nav-text">PyTorch：张量和 Autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">3.1.</span> <span class="nav-text">计算图</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E5%85%B7%E4%BD%93%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">3.1.1.</span> <span class="nav-text">一个具体的例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%B6%E5%AD%90%E5%BC%A0%E9%87%8F"><span class="nav-number">3.1.2.</span> <span class="nav-text">叶子张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#inplace-%E6%93%8D%E4%BD%9C"><span class="nav-number">3.1.3.</span> <span class="nav-text">inplace 操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%9B%BE%EF%BC%8C%E9%9D%99%E6%80%81%E5%9B%BE"><span class="nav-number">3.1.4.</span> <span class="nav-text">动态图，静态图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.1.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-1"><span class="nav-number">3.2.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch%EF%BC%9A%E5%AE%9A%E4%B9%89%E6%96%B0%E7%9A%84-Autograd-%E5%87%BD%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">PyTorch：定义新的 Autograd 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#staticmethod%E5%92%8C-classmethod%E7%9A%84%E7%94%A8%E6%B3%95"><span class="nav-number">4.1.</span> <span class="nav-text">@staticmethod和@classmethod的用法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%B2%E8%A7%A3%E4%B8%80"><span class="nav-number">4.1.1.</span> <span class="nav-text">讲解一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%B2%E8%A7%A3%E4%BA%8C"><span class="nav-number">4.1.2.</span> <span class="nav-text">讲解二</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#with-torch-no-grad-%E8%AF%A6%E8%A7%A3"><span class="nav-number">4.2.</span> <span class="nav-text">with torch.no_grad() 详解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch%EF%BC%9Ann"><span class="nav-number">5.</span> <span class="nav-text">PyTorch：nn</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <a href = "/about/">
    <img class="site-author-image" itemprop="image" alt="Bo"
      src="/images/avatar.jpg">
    </a>
  <p class="site-author-name" itemprop="name">Bo</p>
  <div class="site-description" itemprop="description">I can do all things through Christ who strengthens me!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/GH4Bo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;GH4Bo" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kidsskiss@gmail.com" title="E-Mail → mailto:kidsskiss@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://z2bns.github.io/" title="https:&#x2F;&#x2F;z2bns.github.io&#x2F;" rel="noopener" target="_blank">点击访问大神博客</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 1997 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="far fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bo</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'xvPpIC0fTJw0SCzjTX3RDtjT-gzGzoHsz',
      appKey     : 'GcGmxlL5sRKzOL7wPQUDeCcm',
      placeholder: "请多多指教。。。",
      avatar     : 'monsterid',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"superSample":2,"width":300,"height":600,"position":"left","hOffset":0,"vOffset":-100},"mobile":{"show":false,"scale":0.05},"react":{"opacityDefault":0.7,"opacityOnHover":0.2}});</script></body>
</html>
